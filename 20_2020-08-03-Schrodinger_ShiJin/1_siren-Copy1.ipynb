{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schrodinger equation\n",
    "---\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\varepsilon u_t - i\\frac{\\epsilon^2}{2}\\Delta u + i V(x)u = 0, \\ t \\in \\mathbb{R}, x\\in \\mathbb{R}^d, \\\\\n",
    "u(x, t = 0) = u_0(x),\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $V$ is a given electrostatic potential, $0 < \\varepsilon \\ll 1$.\n",
    "\n",
    "---\n",
    "\n",
    "Example 2 in Shi Jin's paper:\n",
    "$$\n",
    "u(x, 0) = \\sqrt{n_0(x)}e^{iS_0(x)/\\varepsilon},\n",
    "$$\n",
    "(3.9-3.10)\n",
    "$$\n",
    "n_0(x) = (e^{-25(x-0.5)^2})^2, \\ S_0(x) = 0.2(x^2 - x).\n",
    "$$\n",
    "\n",
    "Periodic BC.\n",
    "\n",
    "---\n",
    "\n",
    "Consider the real and imag part of $u$, i.e., \n",
    "$$\n",
    "u(x, t) = p(x, t) + iq(x, t),\n",
    "$$\n",
    "then\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\varepsilon p_t+\\frac{\\varepsilon^2}{2}q_{xx} - V(x)q = 0, \\\\\n",
    "&\\varepsilon q_t-\\frac{\\varepsilon^2}{2}p_{xx} + V(x)p = 0,\n",
    "\\end{aligned}\n",
    "$$\n",
    "with ic\n",
    "$$\n",
    "p(x, 0) = e^{-25(x-0.5)^2}\\cos(\\frac{0.2(x^2-x)}{\\varepsilon}), \\ q(x, 0) = e^{-25(x-0.5)^2}\\sin(\\frac{0.2(x^2-x)}{\\varepsilon}),\n",
    "$$\n",
    "and bc\n",
    "$$\n",
    "p(0, t) = p(1, t), q(0, t) = q(1, t), p_x(0, t) = p_x(1, t), q_x(0, t) = q_x(1, t).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"1_siren\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax, jax.nn\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "from jax.experimental import optimizers\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(\"../../\")\n",
    "\t\n",
    "from Seismic_wave_inversion_PINN.data_utils import *\n",
    "from Seismic_wave_inversion_PINN.jax_model import *\n",
    "\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "key = random.PRNGKey(0)\n",
    "key, subkey = random.split(key, 2)\n",
    "\n",
    "layers = [2] + [128]*4 + [2] # (x, t) -> (u, v)\n",
    "c0 = 1.0\n",
    "w0 = 10.0\n",
    "w1 = 1.0\n",
    "lambda_0 = 1e-8\n",
    "direct_params = init_siren_params(subkey, layers, c0, w0, w1)\n",
    "\n",
    "domain = jnp.array([[0., 0.], [1., 1.]])\n",
    "epsilon = 1.0\n",
    "V = 1.0\n",
    "\n",
    "@jax.jit\n",
    "def model(params, xt):\n",
    "\t# first, normalize to [-1, 1]\n",
    "\txt = 2.0*(xt - domain[0, :])/(domain[1, :]-domain[0, :]) - 1.0\n",
    "\tfor w, b in params[:-1]:\n",
    "\t\txt = jnp.sin(jnp.dot(xt, w) + b)\n",
    "\treturn jnp.dot(xt, params[-1][0]) + params[-1][1]\n",
    "\n",
    "jacobian = jacrev_fn(model)\n",
    "hessian = hessian_fn(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "metaloss = mse\n",
    "\n",
    "jit_conservation = lambda i: jax.partial(jax.jit, static_argnums = i)\n",
    "\n",
    "@jax.jit\n",
    "def loss_fn_(params, batch):\n",
    "\tcollocation, dirichlet, periodic_bc = batch[\"collocation\"], batch[\"dirichlet\"], batch[\"periodic_bc\"]\n",
    "\tdirect_params = params\n",
    "\t\n",
    "\tuv_c = model(direct_params, jnp.hstack([collocation.x, collocation.t]))\n",
    "\tu_c, v_c = uv_c[:, 0:1], uv_c[:, 1:2]\n",
    "\t\n",
    "\t# jacobian[i] = [[du/dx, du/dt],\n",
    "\t#                [dv/dx, dv/dt]]\n",
    "\t# i: the i^th input\n",
    "\tduv_dxt_c = jacobian(direct_params, jnp.hstack([collocation.x, collocation.t]))\n",
    "\tdu_dt_c, dv_dt_c = duv_dxt_c[:, 0:1, 1], duv_dxt_c[:, 1:2, 1]\n",
    "\t\n",
    "\t# hessian[i] = [\n",
    "    #\t\t\t\t[[du/dxx, du/dxy],\n",
    "\t#                [du/dxy, du/dyy]],\n",
    "\t#               [[dv/dxx, dv/dxy],\n",
    "\t#                [dv/dxy, dv/dyy]]\n",
    "\t#              ]\n",
    "\tduv_dxxtt_c = hessian(direct_params, jnp.hstack([collocation.x, collocation.t]))\n",
    "\tdu_dxx_c, dv_dxx_c = duv_dxxtt_c[:, 0:1, 0, 0], duv_dxxtt_c[:, 1:2, 0, 0] \n",
    "\t\n",
    "\tuv_l = model(direct_params, jnp.hstack([periodic_bc.l, periodic_bc.t]))\n",
    "\tuv_r = model(direct_params, jnp.hstack([periodic_bc.r, periodic_bc.t]))\n",
    "\tu_l, v_l = uv_l[:, 0:1], uv_l[:, 1:2]\n",
    "\tu_r, v_r = uv_r[:, 0:1], uv_r[:, 1:2]\n",
    "\t\n",
    "\tduv_dxt_l = jacobian(direct_params, jnp.hstack([periodic_bc.l, periodic_bc.t]))\n",
    "\tduv_dxt_r = jacobian(direct_params, jnp.hstack([periodic_bc.r, periodic_bc.t]))\n",
    "\tdu_dx_l, dv_dx_l = duv_dxt_l[:, 0:1, 0], duv_dxt_l[:, 1:2, 0]\n",
    "\tdu_dx_r, dv_dx_r = duv_dxt_r[:, 0:1, 0], duv_dxt_r[:, 1:2, 0]\n",
    "\t\t\n",
    "\tuv_d = model(direct_params, jnp.hstack([dirichlet.x, dirichlet.t]))\n",
    "\tu_d, v_d = uv_d[:, 0:1], uv_d[:, 1:2]\n",
    "\n",
    "\t\n",
    "\tloss_c1 = metaloss(du_dt_c + 0.5*dv_dxx_c + (u_c**2 + v_c**2)*v_c, 0)\n",
    "\tloss_c2 = metaloss(-dv_dt_c + 0.5*du_dxx_c + (u_c**2 + v_c**2)*u_c, 0)\n",
    "\tloss_c = loss_c1 + loss_c2\n",
    "\t\n",
    "\tloss_d1 = metaloss(u_d, dirichlet.u)\n",
    "\tloss_d2 = metaloss(v_d, dirichlet.v)\n",
    "\tloss_d = loss_d1 + loss_d2\n",
    "\t\n",
    "\tloss_pbc_d1 = metaloss(u_l, u_r)\n",
    "\tloss_pbc_d2 = metaloss(v_l, v_r)\n",
    "\tloss_pbc_n1 = metaloss(du_dx_l, du_dx_r)\n",
    "\tloss_pbc_n2 = metaloss(dv_dx_l, dv_dx_r)\n",
    "\tloss_pbc_d = loss_pbc_d1 + loss_pbc_d2\n",
    "\tloss_pbc_n = loss_pbc_n1 + loss_pbc_n2\n",
    "\t\n",
    "\treturn loss_c, loss_d, loss_pbc_d, loss_pbc_n\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def loss_fn(params, batch):\n",
    "\tw = batch[\"weights\"]\n",
    "\tloss_c, loss_d, loss_pbc_d, loss_pbc_n = loss_fn_(params, batch)\n",
    "\treturn w[\"c\"]*loss_c + w[\"d\"]*loss_d + w[\"pbc_d\"]*loss_pbc_d + w[\"pbc_n\"]*loss_pbc_n\\\n",
    "\t\t\t+ l2_regularization(params, lambda_0)\n",
    "\n",
    "@jax.jit\n",
    "def step(i, opt_state, batch):\n",
    "\tparams = get_params(opt_state)\n",
    "\tgrad = jax.grad(loss_fn, 0)(params, batch)\n",
    "\treturn opt_update(i, grad, opt_state)\n",
    "\n",
    "@jax.jit\n",
    "def evaluate(params, batch):\n",
    "\tw = batch[\"weights\"]\n",
    "\tloss_c, loss_d, loss_pbc_d, loss_pbc_n = loss_fn_(params, batch)\n",
    "\treturn w[\"c\"]*loss_c + w[\"d\"]*loss_d + w[\"pbc_d\"]*loss_pbc_d + w[\"pbc_n\"]*loss_pbc_n, \\\n",
    "\t\t\tloss_c, loss_d, loss_pbc_d, loss_pbc_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(1)\n",
    "key, *subkeys = random.split(key, 4)\n",
    "\n",
    "from scipy.io import loadmat\n",
    "data = loadmat(\"../19_2020-08-01_Schrodinger/NLS.mat\")\n",
    "x, t, h = data[\"x\"].reshape((-1, 1)), data[\"tt\"].reshape((-1, 1)), data[\"uu\"].T\n",
    "u, v = np.real(h), np.imag(h)\n",
    "\n",
    "import pickle\n",
    "with open(\"../19_2020-08-01_Schrodinger/dataset_train.pkl\", \"rb\") as file:\n",
    "\t[ix_i, ix_b, xt_c] = pickle.load(file)\n",
    "\t\n",
    "# # ic\n",
    "# n_i = 50\n",
    "# ix_i = random.choice(subkeys[0], jnp.arange(len(x)), shape = (n_i, ), replace = False)\n",
    "x_i = x[ix_i, :]\n",
    "t_i = np.zeros_like(x_i)\n",
    "u_i = u[0, ix_i].reshape((-1, 1))\n",
    "v_i = v[0, ix_i].reshape((-1, 1))\n",
    "\n",
    "# # bc\n",
    "# n_b = 50\n",
    "# ix_b = random.choice(subkeys[1], jnp.arange(len(t)), shape = (n_b, ), replace = False)\n",
    "t_b = t[ix_b, :]\n",
    "x_lb = np.ones_like(t_b)*domain[0, 0]\n",
    "x_rb = np.ones_like(t_b)*domain[1, 0]\n",
    "\n",
    "# n_c = 20000\n",
    "# from pyDOE import lhs\n",
    "# xt_c = lhs(2, n_c)\n",
    "# x_c = transform(xt_c[:, 0:1], *domain[:, 0])\n",
    "# t_c = transform(xt_c[:, 1:2], *domain[:, 1])\n",
    "x_c, t_c = xt_c[:, 0:1], xt_c[:, 1:2]\n",
    "\n",
    "dataset_Dirichlet = namedtuple(\"dataset_Dirichlet\", [\"x\", \"t\", \"u\", \"v\"])\n",
    "dataset_Collocation = namedtuple(\"dataset_Collocation\", [\"x\", \"t\"])\n",
    "dataset_BC = namedtuple(\"dataset_BC\", [\"l\", \"r\", \"t\"])\n",
    "\n",
    "map_to_jnp_array = lambda x: map(lambda arr: jnp.array(arr), x)\n",
    "dirichlet = dataset_Dirichlet(*map_to_jnp_array([x_i, t_i, u_i, v_i]))\n",
    "periodic_bc = dataset_BC(*map_to_jnp_array([x_lb, x_rb, t_b]))\n",
    "collocation = dataset_Collocation(*map_to_jnp_array([jnp.vstack([dirichlet.x, periodic_bc.l, periodic_bc.r, x_c]),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tjnp.vstack([dirichlet.t, periodic_bc.t, periodic_bc.t, t_c])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/08/05, 21:33:35, Iteration: 0, Train Loss: 7.7047e+00, c: 6.5884e+00, d: 9.1159e-01\n",
      "2020/08/05, 21:33:38, Iteration: 100, Train Loss: 4.8824e-01, c: 5.2886e-02, d: 4.3439e-01\n",
      "2020/08/05, 21:33:41, Iteration: 200, Train Loss: 4.0056e-01, c: 8.5346e-02, d: 3.1340e-01\n",
      "2020/08/05, 21:33:44, Iteration: 300, Train Loss: 3.3547e-01, c: 1.0476e-01, d: 2.2678e-01\n",
      "2020/08/05, 21:33:47, Iteration: 400, Train Loss: 2.8152e-01, c: 1.0403e-01, d: 1.7393e-01\n",
      "2020/08/05, 21:33:50, Iteration: 500, Train Loss: 2.2077e-01, c: 8.8546e-02, d: 1.2978e-01\n",
      "2020/08/05, 21:33:53, Iteration: 600, Train Loss: 1.8433e-01, c: 7.5391e-02, d: 1.0669e-01\n",
      "2020/08/05, 21:33:56, Iteration: 700, Train Loss: 1.7345e-01, c: 7.7588e-02, d: 9.2238e-02\n",
      "2020/08/05, 21:33:59, Iteration: 800, Train Loss: 1.4465e-01, c: 6.1974e-02, d: 8.0547e-02\n",
      "2020/08/05, 21:34:02, Iteration: 900, Train Loss: 1.2834e-01, c: 5.5304e-02, d: 7.1116e-02\n",
      "2020/08/05, 21:34:05, Iteration: 1000, Train Loss: 1.1908e-01, c: 5.4208e-02, d: 6.2036e-02\n",
      "2020/08/05, 21:34:08, Iteration: 1100, Train Loss: 1.0187e-01, c: 4.4898e-02, d: 5.4743e-02\n",
      "2020/08/05, 21:34:11, Iteration: 1200, Train Loss: 9.3151e-02, c: 4.1181e-02, d: 4.9575e-02\n",
      "2020/08/05, 21:34:14, Iteration: 1300, Train Loss: 8.6901e-02, c: 3.8509e-02, d: 4.5913e-02\n",
      "2020/08/05, 21:34:17, Iteration: 1400, Train Loss: 8.9468e-02, c: 4.3691e-02, d: 4.3058e-02\n",
      "2020/08/05, 21:34:20, Iteration: 1500, Train Loss: 8.3038e-02, c: 3.8014e-02, d: 4.2377e-02\n",
      "2020/08/05, 21:34:23, Iteration: 1600, Train Loss: 8.0914e-02, c: 3.8494e-02, d: 3.9401e-02\n",
      "2020/08/05, 21:34:26, Iteration: 1700, Train Loss: 7.2998e-02, c: 3.2519e-02, d: 3.8253e-02\n",
      "2020/08/05, 21:34:29, Iteration: 1800, Train Loss: 7.0351e-02, c: 3.1171e-02, d: 3.6984e-02\n",
      "2020/08/05, 21:34:33, Iteration: 1900, Train Loss: 6.8740e-02, c: 3.0305e-02, d: 3.6251e-02\n",
      "2020/08/05, 21:34:36, Iteration: 2000, Train Loss: 6.7311e-02, c: 2.9590e-02, d: 3.5530e-02\n",
      "2020/08/05, 21:34:39, Iteration: 2100, Train Loss: 6.6338e-02, c: 2.9002e-02, d: 3.5098e-02\n",
      "2020/08/05, 21:34:42, Iteration: 2200, Train Loss: 6.5262e-02, c: 2.8545e-02, d: 3.4518e-02\n",
      "2020/08/05, 21:34:45, Iteration: 2300, Train Loss: 7.2151e-02, c: 3.3623e-02, d: 3.4584e-02\n",
      "2020/08/05, 21:34:49, Iteration: 2400, Train Loss: 6.5513e-02, c: 2.9424e-02, d: 3.3990e-02\n",
      "2020/08/05, 21:34:52, Iteration: 2500, Train Loss: 6.3780e-02, c: 2.7832e-02, d: 3.3720e-02\n",
      "2020/08/05, 21:34:55, Iteration: 2600, Train Loss: 6.2832e-02, c: 2.7197e-02, d: 3.3387e-02\n",
      "2020/08/05, 21:34:58, Iteration: 2700, Train Loss: 6.1802e-02, c: 2.6485e-02, d: 3.2962e-02\n",
      "2020/08/05, 21:35:02, Iteration: 2800, Train Loss: 6.1575e-02, c: 2.6447e-02, d: 3.2840e-02\n",
      "2020/08/05, 21:35:05, Iteration: 2900, Train Loss: 6.2586e-02, c: 2.7528e-02, d: 3.2781e-02\n",
      "2020/08/05, 21:35:08, Iteration: 3000, Train Loss: 6.0675e-02, c: 2.6116e-02, d: 3.2226e-02\n",
      "2020/08/05, 21:35:11, Iteration: 3100, Train Loss: 6.1486e-02, c: 2.6402e-02, d: 3.2338e-02\n",
      "2020/08/05, 21:35:14, Iteration: 3200, Train Loss: 5.9644e-02, c: 2.5443e-02, d: 3.1921e-02\n",
      "2020/08/05, 21:35:18, Iteration: 3300, Train Loss: 7.3965e-02, c: 3.9452e-02, d: 3.2029e-02\n",
      "2020/08/05, 21:35:21, Iteration: 3400, Train Loss: 5.9162e-02, c: 2.5218e-02, d: 3.1659e-02\n",
      "2020/08/05, 21:35:24, Iteration: 3500, Train Loss: 5.8661e-02, c: 2.4962e-02, d: 3.1416e-02\n",
      "2020/08/05, 21:35:27, Iteration: 3600, Train Loss: 5.9187e-02, c: 2.5402e-02, d: 3.1310e-02\n",
      "2020/08/05, 21:35:31, Iteration: 3700, Train Loss: 6.1529e-02, c: 2.7534e-02, d: 3.1403e-02\n",
      "2020/08/05, 21:35:34, Iteration: 3800, Train Loss: 5.7962e-02, c: 2.4580e-02, d: 3.1049e-02\n",
      "2020/08/05, 21:35:37, Iteration: 3900, Train Loss: 5.7454e-02, c: 2.4304e-02, d: 3.0850e-02\n",
      "2020/08/05, 21:35:40, Iteration: 4000, Train Loss: 5.7385e-02, c: 2.4289e-02, d: 3.0706e-02\n",
      "2020/08/05, 21:35:44, Iteration: 4100, Train Loss: 6.5557e-02, c: 3.1695e-02, d: 3.0587e-02\n",
      "2020/08/05, 21:35:47, Iteration: 4200, Train Loss: 5.6810e-02, c: 2.4086e-02, d: 3.0366e-02\n",
      "2020/08/05, 21:35:50, Iteration: 4300, Train Loss: 5.5978e-02, c: 2.3783e-02, d: 2.9917e-02\n",
      "2020/08/05, 21:35:53, Iteration: 4400, Train Loss: 5.5541e-02, c: 2.3640e-02, d: 2.9642e-02\n",
      "2020/08/05, 21:35:56, Iteration: 4500, Train Loss: 5.5301e-02, c: 2.3715e-02, d: 2.9339e-02\n",
      "2020/08/05, 21:36:00, Iteration: 4600, Train Loss: 5.4663e-02, c: 2.3470e-02, d: 2.8922e-02\n",
      "2020/08/05, 21:36:03, Iteration: 4700, Train Loss: 6.2392e-02, c: 2.9629e-02, d: 2.9619e-02\n",
      "2020/08/05, 21:36:06, Iteration: 4800, Train Loss: 5.6194e-02, c: 2.4484e-02, d: 2.8776e-02\n",
      "2020/08/05, 21:36:09, Iteration: 4900, Train Loss: 5.4056e-02, c: 2.3568e-02, d: 2.8262e-02\n",
      "2020/08/05, 21:36:12, Iteration: 5000, Train Loss: 5.2974e-02, c: 2.3044e-02, d: 2.7732e-02\n",
      "2020/08/05, 21:36:16, Iteration: 5100, Train Loss: 5.4785e-02, c: 2.4802e-02, d: 2.7298e-02\n",
      "2020/08/05, 21:36:19, Iteration: 5200, Train Loss: 5.2496e-02, c: 2.3033e-02, d: 2.7149e-02\n",
      "2020/08/05, 21:36:22, Iteration: 5300, Train Loss: 5.2024e-02, c: 2.3128e-02, d: 2.6764e-02\n",
      "2020/08/05, 21:36:25, Iteration: 5400, Train Loss: 5.0972e-02, c: 2.2654e-02, d: 2.6160e-02\n",
      "2020/08/05, 21:36:28, Iteration: 5500, Train Loss: 5.0591e-02, c: 2.2748e-02, d: 2.5740e-02\n",
      "2020/08/05, 21:36:31, Iteration: 5600, Train Loss: 5.0272e-02, c: 2.2615e-02, d: 2.5457e-02\n",
      "2020/08/05, 21:36:35, Iteration: 5700, Train Loss: 5.0227e-02, c: 2.3046e-02, d: 2.4986e-02\n",
      "2020/08/05, 21:36:38, Iteration: 5800, Train Loss: 5.0252e-02, c: 2.3111e-02, d: 2.4937e-02\n",
      "2020/08/05, 21:36:41, Iteration: 5900, Train Loss: 4.8370e-02, c: 2.2196e-02, d: 2.4003e-02\n",
      "2020/08/05, 21:36:44, Iteration: 6000, Train Loss: 4.6904e-02, c: 2.1674e-02, d: 2.3088e-02\n",
      "2020/08/05, 21:36:47, Iteration: 6100, Train Loss: 4.5729e-02, c: 2.1272e-02, d: 2.2297e-02\n",
      "2020/08/05, 21:36:51, Iteration: 6200, Train Loss: 5.2117e-02, c: 2.6445e-02, d: 2.2909e-02\n",
      "2020/08/05, 21:36:54, Iteration: 6300, Train Loss: 4.2913e-02, c: 2.0189e-02, d: 2.0476e-02\n",
      "2020/08/05, 21:36:57, Iteration: 6400, Train Loss: 4.2651e-02, c: 2.0483e-02, d: 1.9845e-02\n",
      "2020/08/05, 21:37:00, Iteration: 6500, Train Loss: 4.0385e-02, c: 1.9311e-02, d: 1.8750e-02\n",
      "2020/08/05, 21:37:03, Iteration: 6600, Train Loss: 4.1904e-02, c: 2.1080e-02, d: 1.8311e-02\n",
      "2020/08/05, 21:37:06, Iteration: 6700, Train Loss: 3.8409e-02, c: 1.8938e-02, d: 1.7060e-02\n",
      "2020/08/05, 21:37:10, Iteration: 6800, Train Loss: 4.6435e-02, c: 2.7630e-02, d: 1.6344e-02\n",
      "2020/08/05, 21:37:13, Iteration: 6900, Train Loss: 3.5168e-02, c: 1.7250e-02, d: 1.5609e-02\n",
      "2020/08/05, 21:37:16, Iteration: 7000, Train Loss: 3.5039e-02, c: 1.7408e-02, d: 1.5206e-02\n",
      "2020/08/05, 21:37:19, Iteration: 7100, Train Loss: 3.3700e-02, c: 1.6968e-02, d: 1.4431e-02\n",
      "2020/08/05, 21:37:22, Iteration: 7200, Train Loss: 3.7360e-02, c: 2.0214e-02, d: 1.4042e-02\n",
      "2020/08/05, 21:37:25, Iteration: 7300, Train Loss: 3.7632e-02, c: 2.1947e-02, d: 1.3140e-02\n",
      "2020/08/05, 21:37:28, Iteration: 7400, Train Loss: 3.2627e-02, c: 1.7376e-02, d: 1.2968e-02\n",
      "2020/08/05, 21:37:32, Iteration: 7500, Train Loss: 2.8450e-02, c: 1.3991e-02, d: 1.2159e-02\n",
      "2020/08/05, 21:37:35, Iteration: 7600, Train Loss: 2.7302e-02, c: 1.3466e-02, d: 1.1596e-02\n",
      "2020/08/05, 21:37:38, Iteration: 7700, Train Loss: 2.6920e-02, c: 1.3554e-02, d: 1.1082e-02\n",
      "2020/08/05, 21:37:41, Iteration: 7800, Train Loss: 2.6669e-02, c: 1.3565e-02, d: 1.0883e-02\n",
      "2020/08/05, 21:37:44, Iteration: 7900, Train Loss: 2.4753e-02, c: 1.2373e-02, d: 1.0268e-02\n",
      "2020/08/05, 21:37:47, Iteration: 8000, Train Loss: 2.4282e-02, c: 1.2254e-02, d: 9.9687e-03\n",
      "2020/08/05, 21:37:51, Iteration: 8100, Train Loss: 2.4523e-02, c: 1.2683e-02, d: 9.8417e-03\n",
      "2020/08/05, 21:37:54, Iteration: 8200, Train Loss: 2.2640e-02, c: 1.1449e-02, d: 9.2287e-03\n",
      "2020/08/05, 21:37:57, Iteration: 8300, Train Loss: 2.1993e-02, c: 1.1211e-02, d: 8.8425e-03\n",
      "2020/08/05, 21:38:00, Iteration: 8400, Train Loss: 4.1034e-02, c: 2.9112e-02, d: 8.6833e-03\n",
      "2020/08/05, 21:38:03, Iteration: 8500, Train Loss: 2.1596e-02, c: 1.1385e-02, d: 8.3056e-03\n",
      "2020/08/05, 21:38:06, Iteration: 8600, Train Loss: 2.0207e-02, c: 1.0408e-02, d: 7.9581e-03\n",
      "2020/08/05, 21:38:10, Iteration: 8700, Train Loss: 2.5988e-02, c: 1.5714e-02, d: 8.1469e-03\n",
      "2020/08/05, 21:38:13, Iteration: 8800, Train Loss: 2.1240e-02, c: 1.2003e-02, d: 7.3947e-03\n",
      "2020/08/05, 21:38:16, Iteration: 8900, Train Loss: 2.0255e-02, c: 1.0740e-02, d: 7.3701e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/08/05, 21:38:19, Iteration: 9000, Train Loss: 4.1784e-02, c: 3.0615e-02, d: 7.1857e-03\n",
      "2020/08/05, 21:38:22, Iteration: 9100, Train Loss: 1.8888e-02, c: 1.0308e-02, d: 6.8724e-03\n",
      "2020/08/05, 21:38:26, Iteration: 9200, Train Loss: 1.7482e-02, c: 9.2861e-03, d: 6.5467e-03\n",
      "2020/08/05, 21:38:29, Iteration: 9300, Train Loss: 1.9045e-02, c: 1.0876e-02, d: 6.5625e-03\n",
      "2020/08/05, 21:38:32, Iteration: 9400, Train Loss: 1.6744e-02, c: 9.0249e-03, d: 6.1469e-03\n",
      "2020/08/05, 21:38:35, Iteration: 9500, Train Loss: 1.6277e-02, c: 8.8272e-03, d: 5.8897e-03\n",
      "2020/08/05, 21:38:38, Iteration: 9600, Train Loss: 1.9430e-02, c: 1.2092e-02, d: 5.7015e-03\n",
      "2020/08/05, 21:38:41, Iteration: 9700, Train Loss: 2.2622e-02, c: 1.3817e-02, d: 5.7620e-03\n",
      "2020/08/05, 21:38:45, Iteration: 9800, Train Loss: 1.5240e-02, c: 8.4172e-03, d: 5.3892e-03\n",
      "2020/08/05, 21:38:48, Iteration: 9900, Train Loss: 2.5133e-02, c: 1.8097e-02, d: 5.3900e-03\n",
      "2020/08/05, 21:38:51, Iteration: 10000, Train Loss: 1.7306e-02, c: 1.0606e-02, d: 5.3171e-03\n",
      "2020/08/05, 21:38:54, Iteration: 10100, Train Loss: 1.4415e-02, c: 8.0779e-03, d: 4.9711e-03\n",
      "2020/08/05, 21:38:57, Iteration: 10200, Train Loss: 2.3735e-02, c: 1.7161e-02, d: 4.9789e-03\n",
      "2020/08/05, 21:39:00, Iteration: 10300, Train Loss: 1.5006e-02, c: 8.8502e-03, d: 4.7084e-03\n",
      "2020/08/05, 21:39:04, Iteration: 10400, Train Loss: 1.3157e-02, c: 7.3770e-03, d: 4.5021e-03\n",
      "2020/08/05, 21:39:07, Iteration: 10500, Train Loss: 2.8696e-02, c: 2.2585e-02, d: 4.5214e-03\n",
      "2020/08/05, 21:39:10, Iteration: 10600, Train Loss: 1.3622e-02, c: 7.9390e-03, d: 4.3495e-03\n",
      "2020/08/05, 21:39:13, Iteration: 10700, Train Loss: 1.2458e-02, c: 7.1532e-03, d: 4.1399e-03\n",
      "2020/08/05, 21:39:16, Iteration: 10800, Train Loss: 1.8987e-02, c: 1.0850e-02, d: 3.9722e-03\n",
      "2020/08/05, 21:39:19, Iteration: 10900, Train Loss: 1.2162e-02, c: 7.0037e-03, d: 3.9516e-03\n",
      "2020/08/05, 21:39:23, Iteration: 11000, Train Loss: 1.1437e-02, c: 6.5448e-03, d: 3.7849e-03\n",
      "2020/08/05, 21:39:26, Iteration: 11100, Train Loss: 2.3644e-02, c: 1.8377e-02, d: 3.9810e-03\n",
      "2020/08/05, 21:39:29, Iteration: 11200, Train Loss: 1.3386e-02, c: 8.1517e-03, d: 3.7069e-03\n",
      "2020/08/05, 21:39:32, Iteration: 11300, Train Loss: 1.1019e-02, c: 6.4529e-03, d: 3.5180e-03\n",
      "2020/08/05, 21:39:35, Iteration: 11400, Train Loss: 1.7519e-02, c: 1.2475e-02, d: 3.6478e-03\n",
      "2020/08/05, 21:39:38, Iteration: 11500, Train Loss: 1.0670e-02, c: 6.2776e-03, d: 3.4137e-03\n",
      "2020/08/05, 21:39:41, Iteration: 11600, Train Loss: 1.0154e-02, c: 5.9116e-03, d: 3.2908e-03\n",
      "2020/08/05, 21:39:45, Iteration: 11700, Train Loss: 1.0059e-02, c: 5.8886e-03, d: 3.2256e-03\n",
      "2020/08/05, 21:39:48, Iteration: 11800, Train Loss: 9.8523e-03, c: 5.7461e-03, d: 3.1817e-03\n",
      "2020/08/05, 21:39:51, Iteration: 11900, Train Loss: 9.6848e-03, c: 5.7075e-03, d: 3.0926e-03\n",
      "2020/08/05, 21:39:54, Iteration: 12000, Train Loss: 1.1122e-02, c: 7.3039e-03, d: 2.9294e-03\n",
      "2020/08/05, 21:39:57, Iteration: 12100, Train Loss: 1.0302e-02, c: 6.1344e-03, d: 2.9686e-03\n",
      "2020/08/05, 21:40:00, Iteration: 12200, Train Loss: 1.3379e-02, c: 9.2385e-03, d: 2.8540e-03\n",
      "2020/08/05, 21:40:04, Iteration: 12300, Train Loss: 9.5932e-03, c: 5.8941e-03, d: 2.8668e-03\n",
      "2020/08/05, 21:40:07, Iteration: 12400, Train Loss: 8.9366e-03, c: 5.3619e-03, d: 2.8113e-03\n",
      "2020/08/05, 21:40:10, Iteration: 12500, Train Loss: 9.1154e-03, c: 5.5264e-03, d: 2.8012e-03\n",
      "2020/08/05, 21:40:13, Iteration: 12600, Train Loss: 1.4420e-02, c: 1.0194e-02, d: 2.8321e-03\n",
      "2020/08/05, 21:40:16, Iteration: 12700, Train Loss: 8.4454e-03, c: 5.0931e-03, d: 2.6083e-03\n",
      "2020/08/05, 21:40:20, Iteration: 12800, Train Loss: 9.7159e-03, c: 6.2809e-03, d: 2.6404e-03\n",
      "2020/08/05, 21:40:23, Iteration: 12900, Train Loss: 8.4663e-03, c: 5.2246e-03, d: 2.5598e-03\n",
      "2020/08/05, 21:40:26, Iteration: 13000, Train Loss: 7.6996e-03, c: 4.5518e-03, d: 2.4675e-03\n",
      "2020/08/05, 21:40:29, Iteration: 13100, Train Loss: 7.5894e-03, c: 4.5544e-03, d: 2.4053e-03\n",
      "2020/08/05, 21:40:32, Iteration: 13200, Train Loss: 7.3222e-03, c: 4.3479e-03, d: 2.3563e-03\n",
      "2020/08/05, 21:40:35, Iteration: 13300, Train Loss: 1.6879e-02, c: 1.3230e-02, d: 2.4748e-03\n",
      "2020/08/05, 21:40:38, Iteration: 13400, Train Loss: 6.9049e-03, c: 4.0699e-03, d: 2.2654e-03\n",
      "2020/08/05, 21:40:42, Iteration: 13500, Train Loss: 9.5457e-03, c: 6.6993e-03, d: 2.2194e-03\n",
      "2020/08/05, 21:40:45, Iteration: 13600, Train Loss: 1.0798e-02, c: 7.6322e-03, d: 2.2929e-03\n",
      "2020/08/05, 21:40:48, Iteration: 13700, Train Loss: 6.6154e-03, c: 3.9017e-03, d: 2.1673e-03\n",
      "2020/08/05, 21:40:51, Iteration: 13800, Train Loss: 8.3401e-03, c: 5.4806e-03, d: 2.1966e-03\n",
      "2020/08/05, 21:40:54, Iteration: 13900, Train Loss: 1.0929e-02, c: 7.5496e-03, d: 2.1895e-03\n",
      "2020/08/05, 21:40:57, Iteration: 14000, Train Loss: 6.2062e-03, c: 3.6432e-03, d: 2.0664e-03\n",
      "2020/08/05, 21:41:00, Iteration: 14100, Train Loss: 6.0203e-03, c: 3.5218e-03, d: 2.0227e-03\n",
      "2020/08/05, 21:41:04, Iteration: 14200, Train Loss: 6.4274e-03, c: 3.9276e-03, d: 2.0240e-03\n",
      "2020/08/05, 21:41:07, Iteration: 14300, Train Loss: 6.0334e-03, c: 3.5866e-03, d: 1.9719e-03\n",
      "2020/08/05, 21:41:10, Iteration: 14400, Train Loss: 5.7084e-03, c: 3.3324e-03, d: 1.9255e-03\n",
      "2020/08/05, 21:41:13, Iteration: 14500, Train Loss: 5.6556e-03, c: 3.3363e-03, d: 1.8862e-03\n",
      "2020/08/05, 21:41:16, Iteration: 14600, Train Loss: 1.3286e-02, c: 1.0671e-02, d: 1.9995e-03\n",
      "2020/08/05, 21:41:20, Iteration: 14700, Train Loss: 6.2865e-03, c: 3.9355e-03, d: 1.8973e-03\n",
      "2020/08/05, 21:41:23, Iteration: 14800, Train Loss: 5.4395e-03, c: 3.2270e-03, d: 1.8123e-03\n",
      "2020/08/05, 21:41:26, Iteration: 14900, Train Loss: 1.2885e-02, c: 1.0466e-02, d: 1.8081e-03\n",
      "2020/08/05, 21:41:29, Iteration: 15000, Train Loss: 5.1407e-03, c: 3.0018e-03, d: 1.7530e-03\n",
      "2020/08/05, 21:41:32, Iteration: 15100, Train Loss: 1.7959e-02, c: 1.5530e-02, d: 1.7137e-03\n",
      "2020/08/05, 21:41:35, Iteration: 15200, Train Loss: 5.0685e-03, c: 2.9867e-03, d: 1.6990e-03\n",
      "2020/08/05, 21:41:38, Iteration: 15300, Train Loss: 8.0281e-03, c: 5.7883e-03, d: 1.7066e-03\n",
      "2020/08/05, 21:41:42, Iteration: 15400, Train Loss: 7.7081e-03, c: 5.6373e-03, d: 1.6624e-03\n",
      "2020/08/05, 21:41:45, Iteration: 15500, Train Loss: 1.1253e-02, c: 8.8925e-03, d: 1.7340e-03\n",
      "2020/08/05, 21:41:48, Iteration: 15600, Train Loss: 4.8471e-03, c: 2.8879e-03, d: 1.6059e-03\n",
      "2020/08/05, 21:41:51, Iteration: 15700, Train Loss: 5.0337e-03, c: 3.0859e-03, d: 1.5958e-03\n",
      "2020/08/05, 21:41:54, Iteration: 15800, Train Loss: 5.3026e-03, c: 3.3809e-03, d: 1.5870e-03\n",
      "2020/08/05, 21:41:57, Iteration: 15900, Train Loss: 1.2168e-02, c: 1.0153e-02, d: 1.5319e-03\n",
      "2020/08/05, 21:42:00, Iteration: 16000, Train Loss: 5.4534e-03, c: 3.6031e-03, d: 1.5090e-03\n",
      "2020/08/05, 21:42:04, Iteration: 16100, Train Loss: 8.8100e-03, c: 6.7857e-03, d: 1.4918e-03\n",
      "2020/08/05, 21:42:07, Iteration: 16200, Train Loss: 4.3477e-03, c: 2.5629e-03, d: 1.4828e-03\n",
      "2020/08/05, 21:42:10, Iteration: 16300, Train Loss: 4.6366e-03, c: 2.8179e-03, d: 1.4973e-03\n",
      "2020/08/05, 21:42:13, Iteration: 16400, Train Loss: 6.3325e-03, c: 4.5385e-03, d: 1.4588e-03\n",
      "2020/08/05, 21:42:16, Iteration: 16500, Train Loss: 1.0774e-02, c: 8.8498e-03, d: 1.4971e-03\n",
      "2020/08/05, 21:42:19, Iteration: 16600, Train Loss: 5.5523e-03, c: 3.8700e-03, d: 1.3879e-03\n",
      "2020/08/05, 21:42:22, Iteration: 16700, Train Loss: 4.1822e-03, c: 2.5258e-03, d: 1.3711e-03\n",
      "2020/08/05, 21:42:26, Iteration: 16800, Train Loss: 5.1394e-03, c: 3.4426e-03, d: 1.3569e-03\n",
      "2020/08/05, 21:42:29, Iteration: 16900, Train Loss: 5.1573e-03, c: 3.4813e-03, d: 1.3613e-03\n",
      "2020/08/05, 21:42:32, Iteration: 17000, Train Loss: 4.3081e-03, c: 2.7124e-03, d: 1.3184e-03\n",
      "2020/08/05, 21:42:35, Iteration: 17100, Train Loss: 7.4628e-03, c: 5.8180e-03, d: 1.3269e-03\n",
      "2020/08/05, 21:42:38, Iteration: 17200, Train Loss: 1.1856e-02, c: 1.0014e-02, d: 1.3550e-03\n",
      "2020/08/05, 21:42:41, Iteration: 17300, Train Loss: 7.3826e-03, c: 5.6489e-03, d: 1.3054e-03\n",
      "2020/08/05, 21:42:45, Iteration: 17400, Train Loss: 3.5687e-03, c: 2.0810e-03, d: 1.2472e-03\n",
      "2020/08/05, 21:42:48, Iteration: 17500, Train Loss: 3.7987e-03, c: 2.3050e-03, d: 1.2520e-03\n",
      "2020/08/05, 21:42:51, Iteration: 17600, Train Loss: 3.7053e-03, c: 2.2380e-03, d: 1.2290e-03\n",
      "2020/08/05, 21:42:54, Iteration: 17700, Train Loss: 3.9614e-03, c: 2.5081e-03, d: 1.2060e-03\n",
      "2020/08/05, 21:42:57, Iteration: 17800, Train Loss: 5.3556e-03, c: 3.8517e-03, d: 1.2263e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/08/05, 21:43:00, Iteration: 17900, Train Loss: 3.4001e-03, c: 2.0007e-03, d: 1.1799e-03\n",
      "2020/08/05, 21:43:03, Iteration: 18000, Train Loss: 4.6434e-03, c: 3.2186e-03, d: 1.1554e-03\n",
      "2020/08/05, 21:43:07, Iteration: 18100, Train Loss: 3.2011e-03, c: 1.8680e-03, d: 1.1284e-03\n",
      "2020/08/05, 21:43:10, Iteration: 18200, Train Loss: 4.9940e-03, c: 3.6156e-03, d: 1.1414e-03\n",
      "2020/08/05, 21:43:13, Iteration: 18300, Train Loss: 3.1080e-03, c: 1.8052e-03, d: 1.1026e-03\n",
      "2020/08/05, 21:43:16, Iteration: 18400, Train Loss: 5.2856e-03, c: 3.8284e-03, d: 1.0890e-03\n",
      "2020/08/05, 21:43:19, Iteration: 18500, Train Loss: 4.7053e-03, c: 3.3578e-03, d: 1.0745e-03\n",
      "2020/08/05, 21:43:22, Iteration: 18600, Train Loss: 3.1938e-03, c: 1.9284e-03, d: 1.0572e-03\n",
      "2020/08/05, 21:43:25, Iteration: 18700, Train Loss: 3.1854e-03, c: 1.9496e-03, d: 1.0401e-03\n",
      "2020/08/05, 21:43:28, Iteration: 18800, Train Loss: 3.4986e-03, c: 2.2628e-03, d: 1.0320e-03\n",
      "2020/08/05, 21:43:32, Iteration: 18900, Train Loss: 5.9522e-03, c: 4.6160e-03, d: 1.0233e-03\n",
      "2020/08/05, 21:43:35, Iteration: 19000, Train Loss: 2.9639e-03, c: 1.7796e-03, d: 1.0032e-03\n",
      "2020/08/05, 21:43:38, Iteration: 19100, Train Loss: 1.2608e-02, c: 1.0939e-02, d: 1.0906e-03\n",
      "2020/08/05, 21:43:41, Iteration: 19200, Train Loss: 3.0524e-03, c: 1.9027e-03, d: 9.6797e-04\n",
      "2020/08/05, 21:43:44, Iteration: 19300, Train Loss: 2.7050e-03, c: 1.5835e-03, d: 9.5224e-04\n",
      "2020/08/05, 21:43:47, Iteration: 19400, Train Loss: 3.0281e-03, c: 1.9080e-03, d: 9.3968e-04\n",
      "2020/08/05, 21:43:51, Iteration: 19500, Train Loss: 4.6001e-03, c: 3.4107e-03, d: 9.4424e-04\n",
      "2020/08/05, 21:43:54, Iteration: 19600, Train Loss: 2.6794e-03, c: 1.6069e-03, d: 9.1151e-04\n",
      "2020/08/05, 21:43:57, Iteration: 19700, Train Loss: 6.6158e-03, c: 5.5023e-03, d: 8.9985e-04\n",
      "2020/08/05, 21:44:00, Iteration: 19800, Train Loss: 8.7845e-03, c: 7.4813e-03, d: 9.2662e-04\n",
      "2020/08/05, 21:44:03, Iteration: 19900, Train Loss: 2.5226e-03, c: 1.4964e-03, d: 8.7344e-04\n",
      "2020/08/05, 21:44:06, Iteration: 20000, Train Loss: 2.7115e-03, c: 1.6821e-03, d: 8.6100e-04\n",
      "2020/08/05, 21:44:09, Iteration: 20100, Train Loss: 6.5287e-03, c: 5.3719e-03, d: 8.6040e-04\n",
      "2020/08/05, 21:44:13, Iteration: 20200, Train Loss: 3.3245e-03, c: 2.3191e-03, d: 8.2329e-04\n",
      "2020/08/05, 21:44:16, Iteration: 20300, Train Loss: 5.8893e-03, c: 4.6843e-03, d: 8.3312e-04\n",
      "2020/08/05, 21:44:19, Iteration: 20400, Train Loss: 2.3968e-03, c: 1.4566e-03, d: 8.0361e-04\n",
      "2020/08/05, 21:44:22, Iteration: 20500, Train Loss: 2.4503e-03, c: 1.5115e-03, d: 7.8520e-04\n",
      "2020/08/05, 21:44:25, Iteration: 20600, Train Loss: 2.8160e-03, c: 1.8798e-03, d: 7.7332e-04\n",
      "2020/08/05, 21:44:28, Iteration: 20700, Train Loss: 2.6485e-03, c: 1.7145e-03, d: 7.8019e-04\n",
      "2020/08/05, 21:44:32, Iteration: 20800, Train Loss: 2.5565e-03, c: 1.6571e-03, d: 7.5567e-04\n",
      "2020/08/05, 21:44:35, Iteration: 20900, Train Loss: 2.4109e-03, c: 1.5278e-03, d: 7.4038e-04\n",
      "2020/08/05, 21:44:38, Iteration: 21000, Train Loss: 2.8502e-03, c: 1.9839e-03, d: 7.2423e-04\n",
      "2020/08/05, 21:44:41, Iteration: 21100, Train Loss: 2.3612e-03, c: 1.5159e-03, d: 7.1562e-04\n",
      "2020/08/05, 21:44:44, Iteration: 21200, Train Loss: 3.0549e-03, c: 2.1975e-03, d: 6.9344e-04\n",
      "2020/08/05, 21:44:47, Iteration: 21300, Train Loss: 4.1673e-03, c: 3.2517e-03, d: 7.2750e-04\n",
      "2020/08/05, 21:44:50, Iteration: 21400, Train Loss: 3.2404e-03, c: 2.3579e-03, d: 6.7911e-04\n",
      "2020/08/05, 21:44:54, Iteration: 21500, Train Loss: 2.5754e-03, c: 1.7280e-03, d: 6.7390e-04\n",
      "2020/08/05, 21:44:57, Iteration: 21600, Train Loss: 3.7347e-03, c: 2.8899e-03, d: 6.4651e-04\n",
      "2020/08/05, 21:45:00, Iteration: 21700, Train Loss: 2.0310e-03, c: 1.2744e-03, d: 6.3843e-04\n",
      "2020/08/05, 21:45:03, Iteration: 21800, Train Loss: 3.1872e-03, c: 2.3627e-03, d: 6.4166e-04\n",
      "2020/08/05, 21:45:06, Iteration: 21900, Train Loss: 3.5693e-03, c: 2.7362e-03, d: 6.2228e-04\n",
      "2020/08/05, 21:45:09, Iteration: 22000, Train Loss: 9.4800e-03, c: 8.2656e-03, d: 6.7709e-04\n",
      "2020/08/05, 21:45:13, Iteration: 22100, Train Loss: 2.4557e-03, c: 1.7231e-03, d: 6.0748e-04\n",
      "2020/08/05, 21:45:16, Iteration: 22200, Train Loss: 2.1901e-03, c: 1.4691e-03, d: 5.8845e-04\n",
      "2020/08/05, 21:45:19, Iteration: 22300, Train Loss: 2.6065e-03, c: 1.9078e-03, d: 5.8093e-04\n",
      "2020/08/05, 21:45:22, Iteration: 22400, Train Loss: 3.6508e-03, c: 2.8865e-03, d: 5.5584e-04\n",
      "2020/08/05, 21:45:25, Iteration: 22500, Train Loss: 4.0236e-03, c: 3.3465e-03, d: 5.5183e-04\n",
      "2020/08/05, 21:45:28, Iteration: 22600, Train Loss: 3.0973e-03, c: 2.3803e-03, d: 5.4055e-04\n",
      "2020/08/05, 21:45:32, Iteration: 22700, Train Loss: 2.6747e-03, c: 2.0194e-03, d: 5.3713e-04\n",
      "2020/08/05, 21:45:35, Iteration: 22800, Train Loss: 1.8254e-03, c: 1.2037e-03, d: 5.1582e-04\n",
      "2020/08/05, 21:45:38, Iteration: 22900, Train Loss: 1.8909e-03, c: 1.2786e-03, d: 5.0876e-04\n",
      "2020/08/05, 21:45:41, Iteration: 23000, Train Loss: 2.0150e-03, c: 1.3804e-03, d: 4.8801e-04\n",
      "2020/08/05, 21:45:44, Iteration: 23100, Train Loss: 4.1389e-03, c: 3.4959e-03, d: 4.7888e-04\n",
      "2020/08/05, 21:45:48, Iteration: 23200, Train Loss: 4.0180e-03, c: 3.3244e-03, d: 4.6659e-04\n",
      "2020/08/05, 21:45:51, Iteration: 23300, Train Loss: 1.8711e-03, c: 1.2911e-03, d: 4.6793e-04\n",
      "2020/08/05, 21:45:54, Iteration: 23400, Train Loss: 3.3069e-03, c: 2.6970e-03, d: 4.8384e-04\n",
      "2020/08/05, 21:45:57, Iteration: 23500, Train Loss: 1.8369e-03, c: 1.2864e-03, d: 4.5108e-04\n",
      "2020/08/05, 21:46:00, Iteration: 23600, Train Loss: 1.7837e-03, c: 1.2163e-03, d: 4.3278e-04\n",
      "2020/08/05, 21:46:03, Iteration: 23700, Train Loss: 1.5071e-03, c: 9.8039e-04, d: 4.3134e-04\n",
      "2020/08/05, 21:46:06, Iteration: 23800, Train Loss: 1.9530e-03, c: 1.3959e-03, d: 4.3937e-04\n",
      "2020/08/05, 21:46:10, Iteration: 23900, Train Loss: 3.1276e-03, c: 2.5167e-03, d: 4.1864e-04\n",
      "2020/08/05, 21:46:13, Iteration: 24000, Train Loss: 2.3121e-03, c: 1.7495e-03, d: 4.2238e-04\n",
      "2020/08/05, 21:46:16, Iteration: 24100, Train Loss: 2.8558e-03, c: 2.3099e-03, d: 3.9207e-04\n",
      "2020/08/05, 21:46:19, Iteration: 24200, Train Loss: 2.1423e-03, c: 1.6442e-03, d: 3.9958e-04\n",
      "2020/08/05, 21:46:22, Iteration: 24300, Train Loss: 1.3608e-03, c: 8.9882e-04, d: 3.7965e-04\n",
      "2020/08/05, 21:46:26, Iteration: 24400, Train Loss: 3.9344e-03, c: 3.2037e-03, d: 3.7980e-04\n",
      "2020/08/05, 21:46:29, Iteration: 24500, Train Loss: 2.5306e-03, c: 2.0507e-03, d: 3.6138e-04\n",
      "2020/08/05, 21:46:32, Iteration: 24600, Train Loss: 1.4858e-03, c: 1.0475e-03, d: 3.5380e-04\n",
      "2020/08/05, 21:46:35, Iteration: 24700, Train Loss: 1.4517e-03, c: 1.0177e-03, d: 3.4589e-04\n",
      "2020/08/05, 21:46:38, Iteration: 24800, Train Loss: 1.4796e-03, c: 1.0417e-03, d: 3.3687e-04\n",
      "2020/08/05, 21:46:41, Iteration: 24900, Train Loss: 1.9976e-03, c: 1.5008e-03, d: 3.4055e-04\n",
      "2020/08/05, 21:46:45, Iteration: 25000, Train Loss: 1.7327e-03, c: 1.3139e-03, d: 3.2718e-04\n",
      "2020/08/05, 21:46:48, Iteration: 25100, Train Loss: 1.4438e-03, c: 1.0280e-03, d: 3.1591e-04\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "start_iteration = 0\n",
    "iterations = 50000\n",
    "print_every = 100\n",
    "save_every = 50000\n",
    "batch_size = {\"dirichlet\": 50, \"bc\": 50, \"collocation\": 20150}\n",
    "weights = {\"c\": 1.0, \"d\": 1.0, \"pbc_d\": 1.0, \"pbc_n\": 1.0}\n",
    "\n",
    "key, *subkeys = random.split(key, 4)\n",
    "Dirichlet = Batch_Generator(subkeys[0], dirichlet, batch_size[\"dirichlet\"])\n",
    "Collocation = Batch_Generator(subkeys[1], collocation, batch_size[\"collocation\"])\n",
    "BC = Batch_Generator(subkeys[2], periodic_bc, batch_size[\"bc\"])\n",
    "params = direct_params\n",
    "\n",
    "opt_init, opt_update, get_params = optimizers.adam(lr)\n",
    "opt_state = opt_init(params)\n",
    "hist = {\"iter\": [], \"loss\": []}\n",
    "\n",
    "for iteration in range(start_iteration, start_iteration+iterations+1):\n",
    "\tbatch = {\n",
    "\t\t\"dirichlet\": dataset_Dirichlet(*next(Dirichlet)),\n",
    "\t\t\"collocation\": dataset_Collocation(*next(Collocation)),\n",
    "\t\t\"periodic_bc\": dataset_BC(*next(BC)),\n",
    "\t\t\"weights\": weights\n",
    "\t}\n",
    "\topt_state = step(iteration, opt_state, batch)\n",
    "\tif (iteration-start_iteration) % print_every == 0:\n",
    "\t\tnames = [\"Loss\", \"c\", \"d\"]\n",
    "\t\tparams_ = get_params(opt_state)\n",
    "\t\tlosses = evaluate(params_, batch)\n",
    "\t\tprint(\"{}, Iteration: {}, Train\".format(get_time(), iteration) + \\\n",
    "\t\t\t  ','.join([\" {}: {:.4e}\".format(name, loss) for name, loss in zip(names, losses)]))\n",
    "\t\thist[\"iter\"].append(iteration)\n",
    "\t\thist[\"loss\"].append(losses[0])\n",
    "\tif (iteration-start_iteration) % save_every == 0:\n",
    "\t\tparams_ = np.asarray(get_params(opt_state), dtype = object)\n",
    "\t\tsave_path = \"models/{}/iteration_{}/params.npy\".format(NAME, iteration)\n",
    "\t\tif not os.path.exists(os.path.dirname(save_path)):\n",
    "\t\t\tos.makedirs(os.path.dirname(save_path))\n",
    "\t\tnp.save(save_path, params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "start_iteration += iterations\n",
    "iterations = 10000\n",
    "print_every = 200\n",
    "save_every = 5000\n",
    "batch_size = {\"dirichlet\": 10000, \"bc\": 1000, \"collocation\": 10000}\n",
    "weights = {\"c\": 1e-3, \"d\": 100.0, \"pbc_d\": 1.0, \"pbc_n\": 1.0, \"conservation\": 1e-6, \"energy\": 0}\n",
    "\n",
    "# key, *subkeys = random.split(key, 4)\n",
    "# Dirichlet = Batch_Generator(subkeys[0], dirichlet, batch_size[\"dirichlet\"])\n",
    "# Collocation = Batch_Generator(subkeys[1], collocation, batch_size[\"collocation\"])\n",
    "# BC = Batch_Generator(subkeys[2], periodic_bc, batch_size[\"bc\"])\n",
    "# params = direct_params\n",
    "\n",
    "# opt_init, opt_update, get_params = optimizers.adam(lr)\n",
    "# opt_state = opt_init(params)\n",
    "# hist = {\"iter\": [], \"loss\": []}\n",
    "\n",
    "for iteration in range(start_iteration, start_iteration+iterations+1):\n",
    "\tbatch = {\n",
    "\t\t\"dirichlet\": dataset_Dirichlet(*next(Dirichlet)),\n",
    "\t\t\"collocation\": dataset_Collocation(*next(Collocation)),\n",
    "\t\t\"periodic_bc\": dataset_BC(*next(BC)),\n",
    "\t\t\"weights\": weights\n",
    "\t}\n",
    "\topt_state = step(iteration, opt_state, batch, conservation)\n",
    "\tif (iteration-start_iteration) % print_every == 0:\n",
    "\t\tnames = [\"Loss\", \"c\", \"d\", \"pbc_d\", \"pbc_n\", \"conservation\", \"energy\"]\n",
    "\t\tparams_ = get_params(opt_state)\n",
    "\t\tlosses = evaluate(params_, batch, conservation)\n",
    "\t\tprint(\"{}, Iteration: {}, Train\".format(get_time(), iteration) + \\\n",
    "\t\t\t  ','.join([\" {}: {:.4e}\".format(name, loss) for name, loss in zip(names, losses)]))\n",
    "\t\thist[\"iter\"].append(iteration)\n",
    "\t\thist[\"loss\"].append(losses)\n",
    "\tif (iteration-start_iteration) % save_every == 0:\n",
    "\t\tparams_ = np.asarray(get_params(opt_state), dtype = object)\n",
    "\t\tsave_path = \"models/{}/iteration_{}/params.npy\".format(NAME, iteration)\n",
    "\t\tif not os.path.exists(os.path.dirname(save_path)):\n",
    "\t\t\tos.makedirs(os.path.dirname(save_path))\n",
    "\t\tnp.save(save_path, params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "start_iteration += iterations\n",
    "iterations = 10000\n",
    "print_every = 200\n",
    "save_every = 5000\n",
    "batch_size = {\"dirichlet\": 10000, \"bc\": 1000, \"collocation\": 10000}\n",
    "weights = {\"c\": 1e-3, \"d\": 100.0, \"pbc_d\": 1.0, \"pbc_n\": 1.0, \"conservation\": 1e-6, \"energy\": 1e-8}\n",
    "\n",
    "# key, *subkeys = random.split(key, 4)\n",
    "# Dirichlet = Batch_Generator(subkeys[0], dirichlet, batch_size[\"dirichlet\"])\n",
    "# Collocation = Batch_Generator(subkeys[1], collocation, batch_size[\"collocation\"])\n",
    "# BC = Batch_Generator(subkeys[2], periodic_bc, batch_size[\"bc\"])\n",
    "# params = direct_params\n",
    "\n",
    "# opt_init, opt_update, get_params = optimizers.adam(lr)\n",
    "# opt_state = opt_init(params)\n",
    "# hist = {\"iter\": [], \"loss\": []}\n",
    "\n",
    "for iteration in range(start_iteration, start_iteration+iterations+1):\n",
    "\tbatch = {\n",
    "\t\t\"dirichlet\": dataset_Dirichlet(*next(Dirichlet)),\n",
    "\t\t\"collocation\": dataset_Collocation(*next(Collocation)),\n",
    "\t\t\"periodic_bc\": dataset_BC(*next(BC)),\n",
    "\t\t\"weights\": weights\n",
    "\t}\n",
    "\topt_state = step(iteration, opt_state, batch, conservation)\n",
    "\tif (iteration-start_iteration) % print_every == 0:\n",
    "\t\tnames = [\"Loss\", \"c\", \"d\", \"pbc_d\", \"pbc_n\", \"conservation\", \"energy\"]\n",
    "\t\tparams_ = get_params(opt_state)\n",
    "\t\tlosses = evaluate(params_, batch, conservation)\n",
    "\t\tprint(\"{}, Iteration: {}, Train\".format(get_time(), iteration) + \\\n",
    "\t\t\t  ','.join([\" {}: {:.4e}\".format(name, loss) for name, loss in zip(names, losses)]))\n",
    "\t\thist[\"iter\"].append(iteration)\n",
    "\t\thist[\"loss\"].append(losses)\n",
    "\tif (iteration-start_iteration) % save_every == 0:\n",
    "\t\tparams_ = np.asarray(get_params(opt_state), dtype = object)\n",
    "\t\tsave_path = \"models/{}/iteration_{}/params.npy\".format(NAME, iteration)\n",
    "\t\tif not os.path.exists(os.path.dirname(save_path)):\n",
    "\t\t\tos.makedirs(os.path.dirname(save_path))\n",
    "\t\tnp.save(save_path, params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "start_iteration += iterations\n",
    "iterations = 20000\n",
    "print_every = 200\n",
    "save_every = 5000\n",
    "batch_size = {\"dirichlet\": 10000, \"bc\": 1000, \"collocation\": 10000}\n",
    "weights = {\"c\": 1e-3, \"d\": 100.0, \"pbc_d\": 1.0, \"pbc_n\": 1.0, \"conservation\": 1e-5, \"energy\": 1e-8}\n",
    "\n",
    "# key, *subkeys = random.split(key, 4)\n",
    "# Dirichlet = Batch_Generator(subkeys[0], dirichlet, batch_size[\"dirichlet\"])\n",
    "# Collocation = Batch_Generator(subkeys[1], collocation, batch_size[\"collocation\"])\n",
    "# BC = Batch_Generator(subkeys[2], periodic_bc, batch_size[\"bc\"])\n",
    "# params = direct_params\n",
    "\n",
    "# opt_init, opt_update, get_params = optimizers.adam(lr)\n",
    "# opt_state = opt_init(params)\n",
    "# hist = {\"iter\": [], \"loss\": []}\n",
    "\n",
    "for iteration in range(start_iteration, start_iteration+iterations+1):\n",
    "\tbatch = {\n",
    "\t\t\"dirichlet\": dataset_Dirichlet(*next(Dirichlet)),\n",
    "\t\t\"collocation\": dataset_Collocation(*next(Collocation)),\n",
    "\t\t\"periodic_bc\": dataset_BC(*next(BC)),\n",
    "\t\t\"weights\": weights\n",
    "\t}\n",
    "\topt_state = step(iteration, opt_state, batch, conservation)\n",
    "\tif (iteration-start_iteration) % print_every == 0:\n",
    "\t\tnames = [\"Loss\", \"c\", \"d\", \"pbc_d\", \"pbc_n\", \"conservation\", \"energy\"]\n",
    "\t\tparams_ = get_params(opt_state)\n",
    "\t\tlosses = evaluate(params_, batch, conservation)\n",
    "\t\tprint(\"{}, Iteration: {}, Train\".format(get_time(), iteration) + \\\n",
    "\t\t\t  ','.join([\" {}: {:.4e}\".format(name, loss) for name, loss in zip(names, losses)]))\n",
    "\t\thist[\"iter\"].append(iteration)\n",
    "\t\thist[\"loss\"].append(losses)\n",
    "\tif (iteration-start_iteration) % save_every == 0:\n",
    "\t\tparams_ = np.asarray(get_params(opt_state), dtype = object)\n",
    "\t\tsave_path = \"models/{}/iteration_{}/params.npy\".format(NAME, iteration)\n",
    "\t\tif not os.path.exists(os.path.dirname(save_path)):\n",
    "\t\t\tos.makedirs(os.path.dirname(save_path))\n",
    "\t\tnp.save(save_path, params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "start_iteration += iterations\n",
    "iterations = 20000\n",
    "print_every = 200\n",
    "save_every = 5000\n",
    "batch_size = {\"dirichlet\": 10000, \"bc\": 1000, \"collocation\": 10000}\n",
    "weights = {\"c\": 1e-3, \"d\": 100.0, \"pbc_d\": 1.0, \"pbc_n\": 1.0, \"conservation\": 1e-3, \"energy\": 1e-7}\n",
    "\n",
    "# key, *subkeys = random.split(key, 4)\n",
    "# Dirichlet = Batch_Generator(subkeys[0], dirichlet, batch_size[\"dirichlet\"])\n",
    "# Collocation = Batch_Generator(subkeys[1], collocation, batch_size[\"collocation\"])\n",
    "# BC = Batch_Generator(subkeys[2], periodic_bc, batch_size[\"bc\"])\n",
    "# params = direct_params\n",
    "\n",
    "# opt_init, opt_update, get_params = optimizers.adam(lr)\n",
    "# opt_state = opt_init(params)\n",
    "# hist = {\"iter\": [], \"loss\": []}\n",
    "\n",
    "for iteration in range(start_iteration, start_iteration+iterations+1):\n",
    "\tbatch = {\n",
    "\t\t\"dirichlet\": dataset_Dirichlet(*next(Dirichlet)),\n",
    "\t\t\"collocation\": dataset_Collocation(*next(Collocation)),\n",
    "\t\t\"periodic_bc\": dataset_BC(*next(BC)),\n",
    "\t\t\"weights\": weights\n",
    "\t}\n",
    "\topt_state = step(iteration, opt_state, batch, conservation)\n",
    "\tif (iteration-start_iteration) % print_every == 0:\n",
    "\t\tnames = [\"Loss\", \"c\", \"d\", \"pbc_d\", \"pbc_n\", \"conservation\", \"energy\"]\n",
    "\t\tparams_ = get_params(opt_state)\n",
    "\t\tlosses = evaluate(params_, batch, conservation)\n",
    "\t\tprint(\"{}, Iteration: {}, Train\".format(get_time(), iteration) + \\\n",
    "\t\t\t  ','.join([\" {}: {:.4e}\".format(name, loss) for name, loss in zip(names, losses)]))\n",
    "\t\thist[\"iter\"].append(iteration)\n",
    "\t\thist[\"loss\"].append(losses)\n",
    "\tif (iteration-start_iteration) % save_every == 0:\n",
    "\t\tparams_ = np.asarray(get_params(opt_state), dtype = object)\n",
    "\t\tsave_path = \"models/{}/iteration_{}/params.npy\".format(NAME, iteration)\n",
    "\t\tif not os.path.exists(os.path.dirname(save_path)):\n",
    "\t\t\tos.makedirs(os.path.dirname(save_path))\n",
    "\t\tnp.save(save_path, params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "start_iteration += iterations\n",
    "iterations = 50000\n",
    "print_every = 200\n",
    "save_every = 5000\n",
    "batch_size = {\"dirichlet\": 10000, \"bc\": 1000, \"collocation\": 10000}\n",
    "weights = {\"c\": 1e-1, \"d\": 100.0, \"pbc_d\": 1.0, \"pbc_n\": 1.0, \"conservation\": 1.0, \"energy\": 1e-5}\n",
    "\n",
    "# key, *subkeys = random.split(key, 4)\n",
    "# Dirichlet = Batch_Generator(subkeys[0], dirichlet, batch_size[\"dirichlet\"])\n",
    "# Collocation = Batch_Generator(subkeys[1], collocation, batch_size[\"collocation\"])\n",
    "# BC = Batch_Generator(subkeys[2], periodic_bc, batch_size[\"bc\"])\n",
    "# params = direct_params\n",
    "\n",
    "# opt_init, opt_update, get_params = optimizers.adam(lr)\n",
    "# opt_state = opt_init(params)\n",
    "# hist = {\"iter\": [], \"loss\": []}\n",
    "\n",
    "for iteration in range(start_iteration, start_iteration+iterations+1):\n",
    "\tbatch = {\n",
    "\t\t\"dirichlet\": dataset_Dirichlet(*next(Dirichlet)),\n",
    "\t\t\"collocation\": dataset_Collocation(*next(Collocation)),\n",
    "\t\t\"periodic_bc\": dataset_BC(*next(BC)),\n",
    "\t\t\"weights\": weights\n",
    "\t}\n",
    "\topt_state = step(iteration, opt_state, batch, conservation)\n",
    "\tif (iteration-start_iteration) % print_every == 0:\n",
    "\t\tnames = [\"Loss\", \"c\", \"d\", \"pbc_d\", \"pbc_n\", \"conservation\", \"energy\"]\n",
    "\t\tparams_ = get_params(opt_state)\n",
    "\t\tlosses = evaluate(params_, batch, conservation)\n",
    "\t\tprint(\"{}, Iteration: {}, Train\".format(get_time(), iteration) + \\\n",
    "\t\t\t  ','.join([\" {}: {:.4e}\".format(name, loss) for name, loss in zip(names, losses)]))\n",
    "\t\thist[\"iter\"].append(iteration)\n",
    "\t\thist[\"loss\"].append(losses)\n",
    "\tif (iteration-start_iteration) % save_every == 0:\n",
    "\t\tparams_ = np.asarray(get_params(opt_state), dtype = object)\n",
    "\t\tsave_path = \"models/{}/iteration_{}/params.npy\".format(NAME, iteration)\n",
    "\t\tif not os.path.exists(os.path.dirname(save_path)):\n",
    "\t\t\tos.makedirs(os.path.dirname(save_path))\n",
    "\t\tnp.save(save_path, params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "start_iteration += iterations\n",
    "iterations = 50000\n",
    "print_every = 200\n",
    "save_every = 5000\n",
    "batch_size = {\"dirichlet\": 10000, \"bc\": 1000, \"collocation\": 10000}\n",
    "weights = {\"c\": 1.0, \"d\": 1e4, \"pbc_d\": 1e2, \"pbc_n\": 1e3, \"conservation\": 1e2, \"energy\": 1e-3}\n",
    "\n",
    "# key, *subkeys = random.split(key, 4)\n",
    "# Dirichlet = Batch_Generator(subkeys[0], dirichlet, batch_size[\"dirichlet\"])\n",
    "# Collocation = Batch_Generator(subkeys[1], collocation, batch_size[\"collocation\"])\n",
    "# BC = Batch_Generator(subkeys[2], periodic_bc, batch_size[\"bc\"])\n",
    "# params = direct_params\n",
    "\n",
    "# opt_init, opt_update, get_params = optimizers.adam(lr)\n",
    "# opt_state = opt_init(params)\n",
    "# hist = {\"iter\": [], \"loss\": []}\n",
    "\n",
    "for iteration in range(start_iteration, start_iteration+iterations+1):\n",
    "\tbatch = {\n",
    "\t\t\"dirichlet\": dataset_Dirichlet(*next(Dirichlet)),\n",
    "\t\t\"collocation\": dataset_Collocation(*next(Collocation)),\n",
    "\t\t\"periodic_bc\": dataset_BC(*next(BC)),\n",
    "\t\t\"weights\": weights\n",
    "\t}\n",
    "\topt_state = step(iteration, opt_state, batch, conservation)\n",
    "\tif (iteration-start_iteration) % print_every == 0:\n",
    "\t\tnames = [\"Loss\", \"c\", \"d\", \"pbc_d\", \"pbc_n\", \"conservation\", \"energy\"]\n",
    "\t\tparams_ = get_params(opt_state)\n",
    "\t\tlosses = evaluate(params_, batch, conservation)\n",
    "\t\tprint(\"{}, Iteration: {}, Train\".format(get_time(), iteration) + \\\n",
    "\t\t\t  ','.join([\" {}: {:.4e}\".format(name, loss) for name, loss in zip(names, losses)]))\n",
    "\t\thist[\"iter\"].append(iteration)\n",
    "\t\thist[\"loss\"].append(losses)\n",
    "\tif (iteration-start_iteration) % save_every == 0:\n",
    "\t\tparams_ = np.asarray(get_params(opt_state), dtype = object)\n",
    "\t\tsave_path = \"models/{}/iteration_{}/params.npy\".format(NAME, iteration)\n",
    "\t\tif not os.path.exists(os.path.dirname(save_path)):\n",
    "\t\t\tos.makedirs(os.path.dirname(save_path))\n",
    "\t\tnp.save(save_path, params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "start_iteration += iterations\n",
    "iterations = 50000\n",
    "print_every = 200\n",
    "save_every = 5000\n",
    "batch_size = {\"dirichlet\": 10000, \"bc\": 1000, \"collocation\": 10000}\n",
    "weights = {\"c\": 1.0, \"d\": 1e4, \"pbc_d\": 1e2, \"pbc_n\": 1e3, \"conservation\": 1e2, \"energy\": 1e-2}\n",
    "\n",
    "# key, *subkeys = random.split(key, 4)\n",
    "# Dirichlet = Batch_Generator(subkeys[0], dirichlet, batch_size[\"dirichlet\"])\n",
    "# Collocation = Batch_Generator(subkeys[1], collocation, batch_size[\"collocation\"])\n",
    "# BC = Batch_Generator(subkeys[2], periodic_bc, batch_size[\"bc\"])\n",
    "# params = direct_params\n",
    "\n",
    "# opt_init, opt_update, get_params = optimizers.adam(lr)\n",
    "# opt_state = opt_init(params)\n",
    "# hist = {\"iter\": [], \"loss\": []}\n",
    "\n",
    "for iteration in range(start_iteration, start_iteration+iterations+1):\n",
    "\tbatch = {\n",
    "\t\t\"dirichlet\": dataset_Dirichlet(*next(Dirichlet)),\n",
    "\t\t\"collocation\": dataset_Collocation(*next(Collocation)),\n",
    "\t\t\"periodic_bc\": dataset_BC(*next(BC)),\n",
    "\t\t\"weights\": weights\n",
    "\t}\n",
    "\topt_state = step(iteration, opt_state, batch, conservation)\n",
    "\tif (iteration-start_iteration) % print_every == 0:\n",
    "\t\tnames = [\"Loss\", \"c\", \"d\", \"pbc_d\", \"pbc_n\", \"conservation\", \"energy\"]\n",
    "\t\tparams_ = get_params(opt_state)\n",
    "\t\tlosses = evaluate(params_, batch, conservation)\n",
    "\t\tprint(\"{}, Iteration: {}, Train\".format(get_time(), iteration) + \\\n",
    "\t\t\t  ','.join([\" {}: {:.4e}\".format(name, loss) for name, loss in zip(names, losses)]))\n",
    "\t\thist[\"iter\"].append(iteration)\n",
    "\t\thist[\"loss\"].append(losses)\n",
    "\tif (iteration-start_iteration) % save_every == 0:\n",
    "\t\tparams_ = np.asarray(get_params(opt_state), dtype = object)\n",
    "\t\tsave_path = \"models/{}/iteration_{}/params.npy\".format(NAME, iteration)\n",
    "\t\tif not os.path.exists(os.path.dirname(save_path)):\n",
    "\t\t\tos.makedirs(os.path.dirname(save_path))\n",
    "\t\tnp.save(save_path, params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "uv_true = loadmat(\"epsilon_1.0.mat\")[\"u\"].T\n",
    "\n",
    "from matplotlib import animation\n",
    "%matplotlib notebook\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize = (15, 5))\n",
    "lines = []\n",
    "for i in range(3):\n",
    "    line1, = ax[i].plot([], [], lw = 1.5, label = \"true\")\n",
    "    line2, = ax[i].plot([], [], lw = 1.5, label = \"pred\")\n",
    "    lines.extend([line1, line2])\n",
    "    ax[i].set_xlim([0, 1])\n",
    "    ax[i].set_ylim([-1, 1])\n",
    "    ax[i].legend()\n",
    "    ax[i].grid()\n",
    "    \n",
    "def init():\n",
    "\tfor line in lines:\n",
    "\t\tline.set_data([], [])\n",
    "\treturn lines\n",
    "\n",
    "params_ = get_params(opt_state)\n",
    "\n",
    "x_test = jnp.linspace(*domain[:, 0], 64)\n",
    "t_test = jnp.linspace(*domain[:, 1], 101)\n",
    "xt_tests = [tensor_grid([x_test, ti]) for ti in t_test]\n",
    "uv_preds = [model(params_, xt_test) for xt_test in xt_tests]\n",
    "u_preds, v_preds = [uv_pred[:, 0:1] for uv_pred in uv_preds], [uv_pred[:, 1:2] for uv_pred in uv_preds]\n",
    "\n",
    "def animate(i):\n",
    "\tu_pred, v_pred = u_preds[i], v_preds[i]\n",
    "\tu_true, v_true = np.real(uv_true[i, :]), np.imag(uv_true[i, :])\n",
    "\t\n",
    "\tlines[0].set_data(x_test, u_true)\n",
    "\tlines[1].set_data(x_test, u_pred)\n",
    "\tax[0].set_title(\"u, t = {:.2f}\".format(t_test[i]))\n",
    "    \n",
    "\tlines[2].set_data(x_test, v_true)\n",
    "\tlines[3].set_data(x_test, v_pred)\n",
    "\tax[1].set_title(\"v, t = {:.2f}\".format(t_test[i]))\n",
    "\n",
    "\tlines[4].set_data(x_test, np.sqrt(u_true**2+v_true**2))\n",
    "\tlines[5].set_data(x_test, np.sqrt(u_pred**2+v_pred**2))\n",
    "\tax[2].set_title(\"|h|, t = {:.2f}\".format(t_test[i]))\n",
    "\n",
    "\treturn lines\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, frames = len(t_test), interval = 1000, blit = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uv_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
