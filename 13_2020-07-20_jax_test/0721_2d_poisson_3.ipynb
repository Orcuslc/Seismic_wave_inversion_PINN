{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2d poissson equation\n",
    "---\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\frac{d^2 u}{dx^2} + \\frac{d^2 u}{dy^2} = -2\\sin(x)\\sin(y), \\ x, y \\in [-\\pi, \\pi]^2, \\\\\n",
    "&u(x, -\\pi) = u(x, \\pi) = u(-\\pi, y) = u(\\pi, y) = 0.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Solution:\n",
    "$$\n",
    "u(x) = \\sin(x)\\sin(y).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax, flax.nn\n",
    "from flax import jax_utils, optim\n",
    "from flax.training import lr_schedule\n",
    "\n",
    "import jax, jax.nn\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "from jax.experimental import optimizers\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\t\n",
    "from Seismic_wave_inversion_PINN.tf_model_utils import *\n",
    "from Seismic_wave_inversion_PINN.data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_layer_params(m, n, key, scale=1e-2):\n",
    "\tw_key, b_key = random.split(key)\n",
    "\treturn jax.nn.initializers.glorot_uniform()(w_key, (n, m)), jax.nn.initializers.zeros(b_key, (n, ))\n",
    "\n",
    "# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n",
    "def init_network_params(key, layers):\n",
    "\tkeys = random.split(key, len(layers))\n",
    "\treturn [random_layer_params(m, n, k) for m, n, k in zip(layers[:-1], layers[1:], keys)]\n",
    "\n",
    "layers = [2, 128, 128, 128, 128, 1]\n",
    "params = init_network_params(random.PRNGKey(0), layers)\n",
    "\n",
    "@jax.jit\n",
    "def scalar_model(params, x, y):\n",
    "\tx_ = jnp.hstack([x, y])\n",
    "\tfor w, b in params[:-1]:\n",
    "\t\tx_ = jnp.tanh(jnp.dot(w, x_) + b)\n",
    "\treturn jnp.sum(jnp.dot(params[-1][0], x_) + params[-1][1])\n",
    "\n",
    "model = jax.jit(jax.vmap(scalar_model, in_axes = (None, 0, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def mse(pred, true):\n",
    "\treturn jnp.mean(jnp.square(pred - true))\n",
    "\n",
    "@jax.jit\n",
    "def loss_fn(params, batch):\n",
    "\n",
    "\t@jax.jit\n",
    "\tdef scalar_du_dx(x, y):\n",
    "\t\treturn jnp.sum(jax.grad(scalar_model, 1)(params, x, y))\n",
    "\n",
    "\t@jax.jit\n",
    "\tdef scalar_du_dy(x, y):\n",
    "\t\treturn jnp.sum(jax.grad(scalar_model, 2)(params, x, y))\n",
    "\n",
    "\t@jax.jit\n",
    "\tdef scalar_du_dxx(x, y):\n",
    "\t\treturn jnp.sum(jax.grad(scalar_du_dx, 0)(x, y))\n",
    "\n",
    "\t@jax.jit\n",
    "\tdef scalar_du_dyy(x, y):\n",
    "\t\treturn jnp.sum(jax.grad(scalar_du_dy, 1)(x, y))\n",
    "\n",
    "\tdu_dxx = jax.vmap(scalar_du_dxx, in_axes = (0, 0))\n",
    "\tdu_dyy = jax.vmap(scalar_du_dyy, in_axes = (0, 0))\n",
    "\n",
    "\tu_b = model(params, batch[\"b\"][\"x\"], batch[\"b\"][\"y\"])\n",
    "\tloss_c = mse(du_dxx(batch[\"c\"][\"x\"], batch[\"c\"][\"y\"]).reshape((-1, 1)) + du_dyy(batch[\"c\"][\"x\"], batch[\"c\"][\"y\"]).reshape((-1, 1)), -2*jnp.sin(batch[\"c\"][\"x\"])*jnp.sin(batch[\"c\"][\"y\"]))\n",
    "\tloss_br = mse(du_dxx(batch[\"b\"][\"x\"], batch[\"b\"][\"y\"]).reshape((-1, 1)) + du_dyy(batch[\"b\"][\"x\"], batch[\"b\"][\"y\"]).reshape((-1, 1)), -2*jnp.sin(batch[\"b\"][\"x\"])*jnp.sin(batch[\"b\"][\"y\"]))\n",
    "\tloss_bv = mse(u_b, batch[\"b\"][\"u\"])\n",
    "\tloss = loss_c + loss_br + loss_bv\n",
    "\treturn loss\n",
    "\n",
    "# @jax.jit\n",
    "# def step(i, opt_state, opt_update, get_params, batch):\n",
    "# \tparams = get_params(opt_state)\n",
    "# \tgrad = jax.grad(loss_fn, 0)(params, batch)\n",
    "# \treturn opt_update(i, grad, opt_state)\n",
    "\n",
    "step_size = 1e-4\n",
    "@jax.jit\n",
    "def update(params, batch):\n",
    "\tgrads = jax.grad(loss_fn, 0)(params, batch)\n",
    "\treturn [(w-step_size*dw, b-step_size*db) for (w, b), (dw, db) in zip(params, grads)]\n",
    "\n",
    "@jax.jit\n",
    "def evaluate(params, batch):\n",
    "\t@jax.jit\n",
    "\tdef scalar_du_dx(x, y):\n",
    "\t\treturn jnp.sum(jax.grad(scalar_model, 1)(params, x, y))\n",
    "\n",
    "\t@jax.jit\n",
    "\tdef scalar_du_dy(x, y):\n",
    "\t\treturn jnp.sum(jax.grad(scalar_model, 2)(params, x, y))\n",
    "\n",
    "\t@jax.jit\n",
    "\tdef scalar_du_dxx(x, y):\n",
    "\t\treturn jnp.sum(jax.grad(scalar_du_dx, 0)(x, y))\n",
    "\n",
    "\t@jax.jit\n",
    "\tdef scalar_du_dyy(x, y):\n",
    "\t\treturn jnp.sum(jax.grad(scalar_du_dy, 1)(x, y))\n",
    "\n",
    "\tdu_dxx = jax.vmap(scalar_du_dxx, in_axes = (0, 0))\n",
    "\tdu_dyy = jax.vmap(scalar_du_dyy, in_axes = (0, 0))\n",
    "\n",
    "\tu_b = model(params, batch[\"b\"][\"x\"], batch[\"b\"][\"y\"])\n",
    "\tloss_c = mse(du_dxx(batch[\"c\"][\"x\"], batch[\"c\"][\"y\"]).reshape((-1, 1)) + du_dyy(batch[\"c\"][\"x\"], batch[\"c\"][\"y\"]).reshape((-1, 1)), -2*jnp.sin(batch[\"c\"][\"x\"])*jnp.sin(batch[\"c\"][\"y\"]))\n",
    "\tloss_br = mse(du_dxx(batch[\"b\"][\"x\"], batch[\"b\"][\"y\"]).reshape((-1, 1)) + du_dyy(batch[\"b\"][\"x\"], batch[\"b\"][\"y\"]).reshape((-1, 1)), -2*jnp.sin(batch[\"b\"][\"x\"])*jnp.sin(batch[\"b\"][\"y\"]))\n",
    "\tloss_bv = mse(u_b, batch[\"b\"][\"u\"])\n",
    "\tloss = loss_c + loss_br + loss_bv\n",
    "\treturn loss, loss_c, loss_br, loss_bv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(0)\n",
    "key, *subkeys = random.split(key, 3)\n",
    "n_c = 10000\n",
    "x_c = random.uniform(subkeys[0], (n_c, 1), minval = -jnp.pi, maxval = jnp.pi)\n",
    "y_c = random.uniform(subkeys[1], (n_c, 1), minval = -jnp.pi, maxval = jnp.pi)\n",
    "\n",
    "key, *subkeys = random.split(key, 5)\n",
    "n_b = 100\n",
    "x_b = jnp.vstack([random.uniform(subkeys[0], (n_b, 1), minval = -jnp.pi, maxval = jnp.pi),\n",
    "\t\t\t\t  jnp.ones((n_b, 1))*jnp.pi,\n",
    "\t\t\t\t  random.uniform(subkeys[1], (n_b, 1), minval = -jnp.pi, maxval = jnp.pi),\n",
    "\t\t\t\t  jnp.ones((n_b, 1))*-jnp.pi])\n",
    "y_b = jnp.vstack([jnp.ones((n_b, 1))*-jnp.pi,\n",
    "\t\t\t\t  random.uniform(subkeys[2], (n_b, 1), minval = -jnp.pi, maxval = jnp.pi),\n",
    "\t\t\t\t  jnp.ones((n_b, 1))*jnp.pi,\n",
    "\t\t\t\t  random.uniform(subkeys[3], (n_b, 1), minval = -jnp.pi, maxval = jnp.pi)])\n",
    "f = lambda x, y: jnp.sin(x)*jnp.sin(y)\n",
    "u_b = f(x_b, y_b)\n",
    "dataset = {\"c\": {\"x\": x_c, \"y\": y_c},\n",
    "\t\t   \"b\": {\"x\": x_b, \"y\": y_b, \"u\": u_b}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020/07/21, 23:31:34, Iteration: 1000, Loss: 9.8784e-01, c: 9.8472e-01, br: 8.9890e-04, bv: 2.2270e-03\n",
      "2020/07/21, 23:31:41, Iteration: 2000, Loss: 9.8421e-01, c: 9.8184e-01, br: 6.8618e-04, bv: 1.6816e-03\n",
      "2020/07/21, 23:31:47, Iteration: 3000, Loss: 9.8094e-01, c: 9.7904e-01, br: 5.4876e-04, bv: 1.3502e-03\n",
      "2020/07/21, 23:31:54, Iteration: 4000, Loss: 9.7783e-01, c: 9.7619e-01, br: 4.6265e-04, bv: 1.1752e-03\n",
      "2020/07/21, 23:32:00, Iteration: 5000, Loss: 9.7472e-01, c: 9.7318e-01, br: 4.1353e-04, bv: 1.1231e-03\n",
      "2020/07/21, 23:32:07, Iteration: 6000, Loss: 9.7149e-01, c: 9.6992e-01, br: 3.9297e-04, bv: 1.1762e-03\n",
      "2020/07/21, 23:32:14, Iteration: 7000, Loss: 9.6803e-01, c: 9.6630e-01, br: 3.9655e-04, bv: 1.3294e-03\n",
      "2020/07/21, 23:32:20, Iteration: 8000, Loss: 9.6421e-01, c: 9.6220e-01, br: 4.2281e-04, bv: 1.5878e-03\n",
      "2020/07/21, 23:32:27, Iteration: 9000, Loss: 9.5992e-01, c: 9.5748e-01, br: 4.7267e-04, bv: 1.9667e-03\n",
      "2020/07/21, 23:32:34, Iteration: 10000, Loss: 9.5500e-01, c: 9.5196e-01, br: 5.4940e-04, bv: 2.4918e-03\n"
     ]
    }
   ],
   "source": [
    "# opt_init, opt_update, get_params = optimizers.adam(1e-4)\n",
    "# opt_state = opt_init(initial_params)\n",
    "for iteration in range(1, 10001):\n",
    "# \tparams = get_params(opt_state)\n",
    "# \tgrad = jax.grad(loss_fn, 0)(params, dataset)\n",
    "# \topt_state = opt_update(iteration, grad, opt_state)\n",
    "\tparams = update(params, dataset)\n",
    "\tif iteration % 1000 == 0:\n",
    "\t\tnames = (\"Loss\", \"c\", \"br\", \"bv\")\n",
    "# \t\tparams = get_params(opt_state)\n",
    "\t\tprint(\"{}, Iteration: {},\".format(get_time(), iteration) + \\\n",
    "\t\t\t  ','.join([\" {}: {:.4e}\".format(name, loss) for name, loss in zip(names, evaluate(params, dataset))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-718277cd4f60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mxy_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mu_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mu_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxy_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "x_test = jnp.linspace(-jnp.pi, jnp.pi, 100).reshape((-1, 1))\n",
    "y_test = x_test\n",
    "xy_test = tensor_grid([x_test, y_test])\n",
    "\n",
    "u_pred = optimizer.target(xy_test)\n",
    "u_test = f(xy_test[:, 0:1], xy_test[:, 1:2])\n",
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib.cm import cool\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "\n",
    "cmap = cool\n",
    "norm = Normalize(vmin=-1.0, vmax=1.0)\n",
    "\n",
    "X, Y = np.meshgrid(x_test, y_test)\n",
    "fig, ax = plt.subplots(1, 3, figsize = (15, 5))\n",
    "fig.subplots_adjust(right = 1.0)\n",
    "\n",
    "im0 = ax[0].contourf(X, Y, u_test.reshape((len(y_test), len(x_test))), cmap = cmap, norm = norm)\n",
    "ax[0].set_title(\"true\")\n",
    "divider = make_axes_locatable(ax[0])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "mpl.colorbar.ColorbarBase(cax, cmap = cmap, norm = norm, orientation='vertical')\n",
    "\n",
    "norm = mpl.colors.Normalize(vmin=-1.0, vmax=1.0)\n",
    "im1 = ax[1].contourf(X, Y, u_pred.reshape((len(y_test), len(x_test))), cmap = cmap, norm = norm)\n",
    "ax[1].set_title(\"pred\")\n",
    "ax[1].set_xlabel(\"\")\n",
    "divider = make_axes_locatable(ax[1])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "mpl.colorbar.ColorbarBase(cax, cmap = cmap, norm = norm, orientation='vertical')\n",
    "\n",
    "norm = mpl.colors.Normalize(vmin=-1e-2, vmax=1e-2)\n",
    "im2 = ax[2].contourf(X, Y, u_test.reshape((len(y_test), len(x_test)))-u_pred.reshape((len(y_test), len(x_test))), cmap = cmap, norm = norm)\n",
    "ax[2].set_title(\"MSE: {}\".format(np.mean(np.square(u_test - u_pred))))\n",
    "divider = make_axes_locatable(ax[2])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "mpl.colorbar.ColorbarBase(cax, cmap = cmap, norm = norm, orientation='vertical')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
