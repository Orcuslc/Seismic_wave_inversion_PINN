{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "from Seismic_wave_inversion_PINN.tf_model_utils import *\n",
    "from Seismic_wave_inversion_PINN.data_utils import *\n",
    "\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "tf.keras.backend.set_floatx(\"float32\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "\tkeras.layers.Dense(128, input_shape = (1,), activation = \"tanh\", kernel_initializer = \"random_normal\"),\n",
    "\tkeras.layers.Dense(128, activation = \"tanh\", kernel_initializer = \"random_normal\"),\n",
    "\tkeras.layers.Dense(128, activation = \"tanh\", kernel_initializer = \"random_normal\"),\n",
    "\tkeras.layers.Dense(128, activation = \"tanh\", kernel_initializer = \"random_normal\"),\n",
    "\tkeras.layers.Dense(1, use_bias = False)\n",
    "])\n",
    "\n",
    "loss_func = keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_residue_loss(tape, x, y):\n",
    "\tdy_dx = tape.gradient(y, x)\n",
    "\treturn loss_func(dy_dx, 1.0)\n",
    "\n",
    "def step(x):\n",
    "\twith tf.GradientTape(persistent = True) as tape:\n",
    "\t\ttape.watch(x)\n",
    "\t\ty_pred = model(x)\n",
    "\t\tloss = get_residue_loss(tape, x, y_pred)\n",
    "\tgrads = tape.gradient(loss, model.trainable_variables)\n",
    "\toptimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\tdel tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(np.random.random((1000, 1)), dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "AVG epoch time:  0.01430511293411255\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.SGD(lr = 0.01)\n",
    "epochs = 5000\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "for i in range(1, epochs+1):\n",
    "\tstep(x)\n",
    "print(\"AVG epoch time: \", (time.time() - start_time)/epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
