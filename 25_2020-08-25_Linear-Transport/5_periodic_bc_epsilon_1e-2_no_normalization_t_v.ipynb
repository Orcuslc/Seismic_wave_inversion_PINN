{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deterministic Linear Transport equation\n",
    "---\n",
    "Consider the equation\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{aligned}\n",
    "&\\frac{\\partial r}{\\partial t} + v\\frac{\\partial j}{\\partial x} = \\frac{\\sigma(x, z)}{\\epsilon^2}(\\hat{r} - r), \\\\\n",
    "&\\frac{\\partial j}{\\partial t} + \\frac{v}{\\epsilon^2}\\frac{\\partial r}{\\partial x} = - \\frac{\\sigma(x, z)}{\\epsilon^2}j, \n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$\n",
    "where $\\epsilon$ is a small number, $v \\in [0, 1]$, and $\\hat{r} = \\int_{0}^{1} rdv$. \n",
    "\n",
    "It seems to me that $v$ follows a uniform distribution (as Gauss-Legendre quadrature is used to compute the integral).\n",
    "\n",
    "We let $\\sigma(x, z) \\equiv 1$.\n",
    "\n",
    "The initial data are\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&r = \\rho_0 \\left(\\exp\\left(-\\left(\\frac{v-0.75}{T_0}\\right)^2\\right) + \\exp\\left(-\\left(\\frac{v-0.75}{T_0}\\right)^2\\right)\\right), \\\\\n",
    "&j = 0, \\\\\n",
    "&\\rho_0 = 1+0.5\\sin(2\\pi x), \\\\\n",
    "&T_0 = 0.25+0.1\\cos(2\\pi x).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "BC: periodic BC, on \n",
    "$$\n",
    "r, j, r_x, j_x\n",
    "$$\n",
    "\n",
    "The spatiotemporal domain is \n",
    "$$\n",
    "(x, t, v) \\in [0, 1]\\times [0, 0.02] \\times [0, 1].\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"5_periodic_bc_epsilon_1e-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax, jax.nn\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "from jax.experimental import optimizers\n",
    "from jax.ops import index, index_add, index_update\n",
    "\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(\"../../\")\n",
    "\t\n",
    "from Seismic_wave_inversion_PINN.data_utils import *\n",
    "from Seismic_wave_inversion_PINN.jax_model import *\n",
    "\n",
    "from collections import namedtuple\n",
    "# from jax.config import config; config.update(\"jax_enable_x64\", True)\n",
    "# dtype = jnp.float64\n",
    "dtype = jnp.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(1)\n",
    "key, subkey = random.split(key, 2)\n",
    "\n",
    "layers = [4] + [32]*4 + [2] # (x, t, v) -> (r, j)\n",
    "c0 = 1.0\n",
    "w0 = jnp.array([[1.0, 1.0, 1.0, 1.0]]).T\n",
    "w1 = jnp.array([[1.0, 1.0]]) # (w_r, w_j)\n",
    "direct_params = init_siren_params(subkey, layers, c0, w0, w1, dtype)\n",
    "\n",
    "domain = jnp.array([[0., 0., 0.0], [1., 0.02, 1.0]])\n",
    "\n",
    "sigma = 1.0\n",
    "epsilon = 1e-2\n",
    "\n",
    "@jax.jit\n",
    "def model(params, xtv): # for predictions\n",
    "\txtv = (2*xtv - (domain[0, :]+domain[1, :]))/(domain[1, :] - domain[0, :])/2\n",
    "\txtv = jnp.dot(xtv, jnp.array([[1.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]))\n",
    "\txtv = index_update(xtv, index[:,0], jnp.sin(2*jnp.pi*xtv[:,0]))\n",
    "\txtv = index_update(xtv, index[:,1], jnp.cos(2*jnp.pi*xtv[:,1]))\n",
    "\tfor w, b in params[:-1]:\n",
    "\t\txtv = jnp.sin(jnp.dot(xtv, w) + b)\n",
    "\treturn jnp.dot(xtv, params[-1][0]) + params[-1][1]\n",
    "\n",
    "@jax.jit\n",
    "def model_(params, xtv): # for derivatives\n",
    "\txtv = (2*xtv - (domain[0, :]+domain[1, :]))/(domain[1, :] - domain[0, :])/2\n",
    "\txtv = jnp.dot(xtv, jnp.array([[1.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0]]))\n",
    "\txtv = index_update(xtv, index[0], jnp.sin(2*jnp.pi*xtv[0]))\n",
    "\txtv = index_update(xtv, index[1], jnp.cos(2*jnp.pi*xtv[1]))\n",
    "\tfor w, b in params[:-1]:\n",
    "\t\txtv = jnp.sin(jnp.dot(xtv, w) + b)\n",
    "\treturn jnp.dot(xtv, params[-1][0]) + params[-1][1]\n",
    "\n",
    "jacobian = jacrev_fn(model_)\n",
    "hessian = hessian_fn(model_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metaloss = mae\n",
    "\n",
    "static_jit = lambda i: jax.partial(jax.jit, static_argnums = i)\n",
    "\n",
    "@jax.jit\n",
    "def quadrature(params, x, t, v, w):\n",
    "\txt_ = jnp.repeat(jnp.hstack([x, t]), w.shape[0], axis = 0)\n",
    "\tv_ = jnp.tile(v, (x.shape[0], 1))\n",
    "\trj = model(params, jnp.hstack([xt_, v_]))\n",
    "\tr = rj[:, 0:1].reshape((x.shape[0], w.shape[0]))\n",
    "\treturn jnp.dot(r, w)\n",
    "\n",
    "# jacobian[i] = [[dr/dx, dr/dt, dr/dv],\n",
    "#                [dj/dx, dj/dt, dj/dv]]\n",
    "# i: the i^th input\n",
    "\n",
    "# hessian[i] = [\n",
    "#\t\t\t\t[[du/dxx, du/dxy],\n",
    "#                [du/dxy, du/dyy]],\n",
    "#               [[dv/dxx, dv/dxy],\n",
    "#                [dv/dxy, dv/dyy]]\n",
    "#              ]\n",
    "@jax.jit\n",
    "def loss_fn_(params, batch):\n",
    "\tcollocation, dirichlet, quad = batch[\"collocation\"], batch[\"dirichlet\"], batch[\"quad\"]\n",
    "\tdirect_params = params\n",
    "\t\n",
    "\tif collocation[0] is not None:\n",
    "\t\trj_c = model(direct_params, jnp.hstack([collocation.x, collocation.t, collocation.v]))\n",
    "\t\tr_c, j_c = rj_c[:, 0:1], rj_c[:, 1:2]\n",
    "\t\tdrj_dxtv_c = jacobian(direct_params, jnp.hstack([collocation.x, collocation.t, collocation.v]))\n",
    "\t\tdr_dt_c, dj_dt_c = drj_dxtv_c[:, 0:1, 1], drj_dxtv_c[:, 1:2, 1]\n",
    "\t\tdr_dx_c, dj_dx_c = drj_dxtv_c[:, 0:1, 0], drj_dxtv_c[:, 1:2, 0]\n",
    "\t\t\n",
    "\t\t# quad.w: [q, 1]\n",
    "\t\t# quad.v: [q, 1]\n",
    "\t\tr_hat_c = quadrature(direct_params, collocation.x, collocation.t, quad.v, quad.w)\n",
    "\t\t\n",
    "\t\tloss_c1 = metaloss(epsilon**2*(dr_dt_c + collocation.v*dj_dx_c), sigma*(r_hat_c - r_c))\n",
    "\t\tloss_c2 = metaloss(epsilon**2*dj_dt_c + collocation.v*dr_dx_c, -sigma*j_c)\n",
    "\telse:\n",
    "\t\tloss_c1 = loss_c2 = 0\n",
    "        \n",
    "\tif dirichlet[0] is not None:\n",
    "\t\trj_d = model(direct_params, jnp.hstack([dirichlet.x, dirichlet.t, dirichlet.v]))\n",
    "\t\tr_d, j_d = rj_d[:, 0:1], rj_d[:, 1:2]\n",
    "\t\tloss_dr = metaloss(r_d, dirichlet.r)\n",
    "\t\tloss_dj = metaloss(j_d, dirichlet.j)\n",
    "\telse:\n",
    "\t\tloss_dr = loss_dj = 0.0\n",
    "\t\t\n",
    "# \tif bc[0] is not None:\n",
    "# \t\trj_bl = model(direct_params, jnp.hstack([bc.l, bc.t, bc.v]))\n",
    "# \t\trj_br = model(direct_params, jnp.hstack([bc.r, bc.t, bc.v]))\n",
    "# \t\tr_bl, j_bl = rj_bl[:, 0:1], rj_bl[:, 1:2]\n",
    "# \t\tr_br, j_br = rj_br[:, 0:1], rj_br[:, 1:2]\n",
    "# \t\tdrj_dxtv_bl = jacobian(direct_params, jnp.hstack([bc.l, bc.t, bc.v]))\n",
    "# \t\tdrj_dxtv_br = jacobian(direct_params, jnp.hstack([bc.r, bc.t, bc.v]))\n",
    "# \t\tdr_dx_bl, dr_dx_br = drj_dxtv_bl[:, 0:1, 0], drj_dxtv_br[:, 0:1, 0]\n",
    "# \t\tdj_dx_bl, dj_dx_br = drj_dxtv_bl[:, 1:2, 0], drj_dxtv_br[:, 1:2, 0]\n",
    "\t\t\n",
    "# \t\tloss_b1 = metaloss(r_bl, r_br) + metaloss(j_bl, j_br)\n",
    "# \t\tloss_b2 = metaloss(dr_dx_bl, dr_dx_br) + metaloss(dj_dx_bl, dj_dx_br)\n",
    "\t\t\n",
    "# \t\tloss_b1 = metaloss(sigma*j_bl, -bc.v*dr_dx_bl) + metaloss(sigma*j_br, -bc.v*dr_dx_br)\n",
    "# \t\tloss_b2 = metaloss(r_bl - epsilon/sigma*bc.v*dr_dx_bl, 1.0) + metaloss(r_br + epsilon/sigma*bc.v*dr_dx_br, 0.0)\n",
    "\n",
    "\treturn loss_c1, loss_c2, loss_dr, loss_dj\n",
    "\n",
    "@jax.jit\n",
    "def loss_fn(params, batch):\n",
    "\tw = batch[\"weights\"]\n",
    "\tloss_c1, loss_c2, loss_dr, loss_dj = loss_fn_(params, batch)\n",
    "\treturn w[\"c1\"]*loss_c1 + w[\"c2\"]*loss_c2 + w[\"dr\"]*loss_dr + w[\"dj\"]*loss_dj + \\\n",
    "\t\t\tl1_regularization(params, w[\"l1\"]) + l2_regularization(params, w[\"l2\"])\n",
    "\n",
    "@jax.jit\n",
    "def step(i, opt_state, batch):\n",
    "\tparams = get_params(opt_state)\n",
    "\tgrad = jax.grad(loss_fn, 0)(params, batch)\n",
    "\treturn opt_update(i, grad, opt_state)\n",
    "\n",
    "@jax.jit\n",
    "def evaluate(params, batch):\n",
    "\tw = batch[\"weights\"]\n",
    "\tloss_c1, loss_c2, loss_dr, loss_dj = loss_fn_(params, batch)\n",
    "\tl1 = l1_regularization(params, 1.0)\n",
    "\tl2 = l2_regularization(params, 1.0)\n",
    "\treturn w[\"c1\"]*loss_c1 + w[\"c2\"]*loss_c2 + w[\"dr\"]*loss_dr + w[\"dj\"]*loss_dj, \\\n",
    "\t\t\tloss_c1, loss_c2, loss_dr, loss_dj, l1, l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho0_fn = lambda x: 1+0.5*jnp.sin(jnp.pi*2*x)\n",
    "T0_fn = lambda x: 0.25+0.1*jnp.cos(jnp.pi*2*x)\n",
    "r0_fn = lambda x, t, v: rho0_fn(x)*(jnp.exp(-((v - 0.75)/T0_fn(x))**2) + jnp.exp(-((v+0.75)/T0_fn(x))**2))\n",
    "j0_fn = lambda x, t, v: jnp.zeros_like(x)\n",
    "\n",
    "key, *subkeys = random.split(key, 3)\n",
    "\n",
    "n_quad = 16\n",
    "v_quad, w_quad = np.polynomial.legendre.leggauss(n_quad)\n",
    "v_quad = jnp.array(0.5*(v_quad+1), dtype = jnp.float32).reshape((-1, 1))\n",
    "w_quad = jnp.array(0.5*w_quad, dtype = jnp.float32).reshape((-1, 1))\n",
    "\n",
    "n_i = 200\n",
    "x_i = jnp.linspace(*domain[:, 0], n_i)\n",
    "# v_i = jnp.linspace(*domain[:, 2], n_i)\n",
    "v_i = v_quad\n",
    "xv_i = tensor_grid([x_i, v_i])\n",
    "x_i, v_i = xv_i[:, 0:1], xv_i[:, 1:2]\n",
    "t_i = jnp.zeros_like(x_i)\n",
    "r_i = r0_fn(x_i, t_i, v_i)\n",
    "j_i = j0_fn(x_i, t_i, v_i)\n",
    "\n",
    "n_cx = 201\n",
    "n_ct = 100\n",
    "x_c = jnp.linspace(*domain[:, 0], n_cx).reshape((-1, 1))\n",
    "t_c = jnp.linspace(*domain[:, 1], n_ct).reshape((-1, 1))\n",
    "# v_c = jnp.linspace(*domain[:, 2], n_cv).reshape((-1, 1))\n",
    "v_c = v_quad\n",
    "xtv_c = tensor_grid([x_c, t_c, v_c])\n",
    "\n",
    "dataset_Dirichlet = namedtuple(\"dataset_Dirichlet\", [\"x\", \"t\", \"v\", \"r\", \"j\"])\n",
    "dataset_Collocation = namedtuple(\"dataset_Collocation\", [\"x\", \"t\", \"v\"])\n",
    "dataset_Quad = namedtuple(\"dataset_Quad\", [\"w\", \"v\"])\n",
    "\n",
    "dirichlet = dataset_Dirichlet(x_i, t_i, v_i, r_i, j_i)\n",
    "collocation = dataset_Collocation(xtv_c[:, 0:1], xtv_c[:, 1:2], xtv_c[:, 2:3])\n",
    "quad = dataset_Quad(w_quad, v_quad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "params = direct_params\n",
    "opt_init, opt_update, get_params = optimizers.adam(lr)\n",
    "opt_state = opt_init(params)\n",
    "hist = {\"iter\": [], \"loss\": []}\n",
    "\n",
    "batch_size = {\"dirichlet\": 200, \"collocation\": 20100}\n",
    "key, *subkeys = random.split(key, 5)\n",
    "Dirichlet = Batch_Generator(subkeys[0], dirichlet, batch_size[\"dirichlet\"])\n",
    "Collocation = Batch_Generator(subkeys[1], collocation, batch_size[\"collocation\"])\n",
    "\n",
    "start_iteration = 0\n",
    "iterations = 200000\n",
    "print_every = 1000\n",
    "save_every = 10000\n",
    "weights = {\"c1\": 10.0, \"c2\": 10.0, \"dr\": 10.0, \"dj\": 10.0, \"l1\": 1e-8, \"l2\": 1e-8}\n",
    "\n",
    "for iteration in range(start_iteration, start_iteration+iterations+1):\n",
    "\td = next(Dirichlet)\n",
    "\tc = next(Collocation)\n",
    "\tbatch = {\n",
    "\t\t\"dirichlet\": dataset_Dirichlet(*d),\n",
    "\t\t\"collocation\": dataset_Collocation(jnp.vstack([d[0], c[0]]), jnp.vstack([d[1], c[1]]), jnp.vstack([d[2], c[2]])),\n",
    "\t\t\"quad\": quad,\n",
    "\t\t\"weights\": weights,\n",
    "\t}\n",
    "\topt_state = step(iteration, opt_state, batch)\n",
    "\tif (iteration-start_iteration) % print_every == 0:\n",
    "\t\tnames = [\"Loss\", \"c1\", \"c2\", \"dr\", \"dj\", \"l1_reg\", \"l2_reg\"]\n",
    "\t\tparams_ = get_params(opt_state)\n",
    "\t\tbatch = {\n",
    "\t\t\t\"dirichlet\": dataset_Dirichlet(*Dirichlet.dataset),\n",
    "\t\t\t\"collocation\": batch[\"collocation\"],\n",
    "\t\t\t\"quad\": quad,\n",
    "\t\t\t\"weights\": weights\n",
    "\t\t}\n",
    "\t\tlosses = evaluate(params_, batch)\n",
    "\t\tprint(\"{}, Iteration: {}, Train\".format(get_time(), iteration) + \\\n",
    "\t\t\t  ','.join([\" {}: {:.4e}\".format(name, loss) for name, loss in zip(names, losses)]))\n",
    "\t\thist[\"iter\"].append(iteration)\n",
    "\t\thist[\"loss\"].append(losses)\n",
    "\tif (iteration-start_iteration) % save_every == 0:\n",
    "\t\tparams_ = np.asarray(get_params(opt_state), dtype = object)\n",
    "\t\tsave_path = \"models/{}/iteration_{}/params.npy\".format(NAME, iteration)\n",
    "\t\tif not os.path.exists(os.path.dirname(save_path)):\n",
    "\t\t\tos.makedirs(os.path.dirname(save_path))\n",
    "\t\tnp.save(save_path, params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "params_ = get_params(opt_state)\n",
    "\n",
    "data_true = loadmat(\"4_snapshots_epsilon_1e-8.mat\")\n",
    "x_test = data_true[\"x\"]\n",
    "t_test = data_true[\"times\"][0]\n",
    "r_hat_trues = data_true[\"rhos\"].T\n",
    "\n",
    "x_test = jnp.linspace(*domain[:, 0], 200)\n",
    "t_test = jnp.linspace(*domain[:, 1], 2000)\n",
    "xt_tests = [tensor_grid([x_test, ti]) for ti in t_test]\n",
    "\n",
    "r_hat_preds = [quadrature(params_, xt[:, 0:1], xt[:, 1:2], quad.v, quad.w) for xt in xt_tests]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "%matplotlib notebook\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (5, 5))\n",
    "line1, = ax.plot([], [], lw = 1.5, label = \"pred\")\n",
    "line2, = ax.plot([], [], lw = 1.5, label = \"true\")\n",
    "lines = [line1, line2]\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1])\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "    \n",
    "def init():\n",
    "\tfor line in lines:\n",
    "\t\tline.set_data([], [])\n",
    "\treturn lines\n",
    "\n",
    "def animate(i):\n",
    "\tr_hat_pred = r_hat_preds[i]\n",
    "\tlines[0].set_data(x_test, r_hat_pred)\n",
    "\tax.set_title(\"r, t = {:.4f}\".format(t_test[i]))\n",
    "\treturn lines\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, frames = len(t_test), interval = 100, blit = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "data = loadmat(\"epsilon_1e-16.mat\")\n",
    "x_true_, u_true = data[\"x\"], data[\"u\"]\n",
    "x_true = np.zeros_like(u_true)\n",
    "x_true[0] = 0; x_true[-1] = 1\n",
    "x_true[1:-1] = x_true_\n",
    "\n",
    "f, ax = plt.subplots(1, 2, figsize = (10, 5))\n",
    "ax[0].plot(x_test, r_hat_preds[-1], label = \"pred\")\n",
    "ax[1].plot(x_true, u_true, label = \"true\")\n",
    "for i in range(2):\n",
    "\tax[i].legend()\n",
    "\tax[i].grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
