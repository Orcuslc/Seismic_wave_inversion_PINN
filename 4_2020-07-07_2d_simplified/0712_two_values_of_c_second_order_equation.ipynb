{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Acoustic Wave Equation\n",
    "---\n",
    "\n",
    "Consider the 2d acoustic wave equation\n",
    "$$\n",
    "\\frac{\\partial^2 p}{\\partial t^2} = c^2(x, z)\\left(\\frac{\\partial^2 p}{\\partial x^2} + \\frac{\\partial^2 p}{\\partial z^2}\\right) + \\frac{\\partial}{\\partial t}f(x, z, t).\n",
    "$$\n",
    "where $p$ is Pressure, $c$ is Medium Velocity. The source term $f$ is represented by a Ricker wavelet (practically, we regard it as a multiplication of Ricker wavelet in temporal domain with Gaussian dissolve in the spatial domain)\n",
    "$$\n",
    "f(x, z, t) = R(t)N(x, z),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "R(t) = \\left(1-2(\\pi f_0(t-t_0))^2\\right)\\exp\\left(-(\\pi f_0(t-t_0))^2\\right),\n",
    "$$\n",
    "where $f_0 = 20$ is the dominant frequency, $t_0 = 0.05$ is time delay, and\n",
    "$$\n",
    "N(x, z) = \\exp\\left(-\\frac{1}{\\alpha^2}\\left((x-x_s)^2+(z-z_s)^2\\right)\\right).\n",
    "$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Data\n",
    "\n",
    "- $p$: Pressure\n",
    "\n",
    "- All known data are collected in the spatial domain $(x, z) \\in [1, 500]\\times [1, 500]$, with grid size 10 (which means the spatial grids are {10, 20, ..., 500}. The temporal domain is $t\\in [0, 0.5)$, with time step $dt = 0.01$. This gives 50 time steps as recorded, i.e., $t = \\{0, 0.01, \\cdots, 0.49\\}$.\n",
    "\n",
    "- The sources are located at $(x_s, z_s) = \\{50, 100, \\cdots, 450\\} \\times \\{10\\}$.\n",
    "\n",
    "## Target\n",
    "- Recover the direct problem $p$\n",
    "- Recover the inverse problem $c^2$.\n",
    "\n",
    "## Scaling (A simple substitution of variables)\n",
    "\n",
    "We let\n",
    "$$\n",
    "x' = \\frac{x}{x_0}, \\ z' = \\frac{z}{z_0}, \\ t' = \\frac{t}{t_0}, \\ u' = \\frac{u}{u_0}, \\ c' = \\frac{c}{c_0},\n",
    "$$\n",
    "where $x_0, z_0, t_0, u_0, c_0$ are scaling constants to be determined. Then the equation becomes\n",
    "$$\n",
    "\\frac{p_0}{t_0^2}\\frac{\\partial^2 p'}{\\partial t'^2} = c_0^2c'(x_0x', z_0z')\\left(\\frac{p_0}{x_0^2}\\frac{\\partial^2 p'}{\\partial x'^2} + \\frac{p_0}{z_0^2}\\frac{\\partial^2 p'}{\\partial z'^2}\\right) + f(x_0x', z_0z', t_0t').\n",
    "$$\n",
    "\n",
    "In this case, we let\n",
    "$$\n",
    "x_0 = z_0 = c_0 = 1000, \\ t_0 = 1, \\ p_0 = 1,\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Models\n",
    "\n",
    "The inverse model will return just either values of $c$, which needs to be learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"0712_two_values_of_c_second_order_equation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "from Seismic_wave_inversion_PINN.tf_model_utils import *\n",
    "from Seismic_wave_inversion_PINN.data_utils import *\n",
    "\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "tf.keras.backend.set_floatx(\"float32\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def siren_model(layers, c, w0, lambda_1, adaptive = None):\n",
    "\tclass scaled_dense(keras.layers.Layer):\n",
    "\t\tdef __init__(self, units, input_dim, c, w0, lambda_1):\n",
    "\t\t\tsuper(scaled_dense, self).__init__()\n",
    "\t\t\tself.lambda_1 = lambda_1\n",
    "\t\t\tw_init = tf.random_uniform_initializer(-np.sqrt(c/input_dim), np.sqrt(c/input_dim))\n",
    "\t\t\tself.w = tf.Variable(initial_value = w_init(shape = (input_dim, units), dtype = \"float32\"), trainable = True)\n",
    "\t\t\tb_init = tf.zeros_initializer()\n",
    "\t\t\tself.b = tf.Variable(initial_value = b_init(shape=(units,), dtype=\"float32\"), trainable=True)\n",
    "\t\t\tself.w0 = tf.Variable(w0, dtype = \"float32\", trainable = True)\n",
    "\t\t\t\n",
    "\t\tdef call(self, inputs):\n",
    "\t\t\tself.add_loss(self.lambda_1*tf.reduce_sum(tf.abs(self.w)))\n",
    "\t\t\treturn tf.sin(tf.matmul(inputs, self.w)*self.w0 + self.b)\n",
    "\t\n",
    "\twith tf.device(\"/device:GPU:0\"):\n",
    "\t\tmodel = keras.models.Sequential()\n",
    "\t\tif adaptive == \"first\" or adaptive == \"all\":\n",
    "\t\t\tmodel.add(scaled_dense(layers[1], layers[0], c, w0, lambda_1))\n",
    "\t\telse:\n",
    "\t\t\tmodel.add(keras.layers.Dense(layers[1], input_shape = (layers[0], ), activation = K.sin,\n",
    "\t\t\t\t\t\t\t\t\t\tkernel_initializer = keras.initializers.RandomUniform(-w0*np.sqrt(c/layers[0]), w0*np.sqrt(c/layers[0])),\n",
    "\t\t\t\t\t\t\t\t\t\tkernel_regularizer = keras.regularizers.l1(lambda_1)))\n",
    "\t\tfor i in range(1, len(layers)-2):\n",
    "\t\t\tif adaptive == \"all\":\n",
    "\t\t\t\tmodel.add(scaled_dense(layers[i+1], layers[i], c, 1.0, lambda_1))\n",
    "\t\t\telse:\n",
    "\t\t\t\tmodel.add(keras.layers.Dense(layers[i+1], input_shape = (layers[i], ), activation = K.sin,\n",
    "\t\t\t\t\t\t\t\t\tkernel_initializer = keras.initializers.RandomUniform(-np.sqrt(c/layers[i]), np.sqrt(c/layers[i])),\n",
    "\t\t\t\t\t\t\t\t\tkernel_regularizer = keras.regularizers.l1(lambda_1)))\n",
    "\t\tmodel.add(keras.layers.Dense(layers[-1]))\n",
    "\treturn model\n",
    "\n",
    "def tanh_model(layers, adaptive = None, bias = None):\n",
    "\tclass adaptive_dense(keras.layers.Layer):\n",
    "\t\tdef __init__(self, units, input_dim):\n",
    "\t\t\tsuper(adaptive_dense, self).__init__()\n",
    "\t\t\tw_init = keras.initializers.GlorotUniform()\n",
    "\t\t\tself.w = tf.Variable(initial_value = w_init(shape = (input_dim, units), dtype = \"float32\"), trainable = True)\n",
    "\t\t\tb_init = tf.zeros_initializer()\n",
    "\t\t\tself.b = tf.Variable(initial_value = b_init(shape=(units,), dtype=\"float32\"), trainable=True)\n",
    "\t\t\tself.a = tf.Variable(1.0, dtype = \"float32\", trainable = True)\n",
    "\t\t\t\n",
    "\t\tdef call(self, inputs):\n",
    "\t\t\treturn tf.tanh(self.a*(tf.matmul(inputs, self.w) + self.b))\n",
    "\t\n",
    "\twith tf.device(\"/device:GPU:0\"):\n",
    "\t\tmodel = keras.models.Sequential()\n",
    "\t\tif adaptive == \"first\" or adaptive == \"all\":\n",
    "\t\t\tmodel.add(adaptive_dense(layers[1], layers[0]))\n",
    "\t\telse:\n",
    "\t\t\tmodel.add(keras.layers.Dense(layers[1], input_shape = (layers[0], ), activation = \"tanh\"))\n",
    "\t\tfor i in range(1, len(layers)-2):\n",
    "\t\t\tif adaptive == \"all\":\n",
    "\t\t\t\tmodel.add(adaptive_dense(layers[i+1], layers[i]))\n",
    "\t\t\telse:\n",
    "\t\t\t\tmodel.add(keras.layers.Dense(layers[i+1], input_shape = (layers[i], ), activation = \"tanh\"))\n",
    "\t\tif bias:\n",
    "\t\t\tmodel.add(keras.layers.Dense(layers[-1], bias_initializer = keras.initializers.Constant(bias)))\n",
    "\t\telse:\n",
    "\t\t\tmodel.add(keras.layers.Dense(layers[-1]))\n",
    "\treturn model\n",
    "\n",
    "def swish_model(layers, adaptive = None, bias = None):\n",
    "\tclass adaptive_dense(keras.layers.Layer):\n",
    "\t\tdef __init__(self, units, input_dim):\n",
    "\t\t\tsuper(adaptive_dense, self).__init__()\n",
    "\t\t\tw_init = keras.initializers.GlorotUniform()\n",
    "\t\t\tself.w = tf.Variable(initial_value = w_init(shape = (input_dim, units), dtype = \"float32\"), trainable = True)\n",
    "\t\t\tb_init = tf.zeros_initializer()\n",
    "\t\t\tself.b = tf.Variable(initial_value = b_init(shape=(units,), dtype=\"float32\"), trainable=True)\n",
    "\t\t\tself.a = tf.Variable(1.0, dtype = \"float32\", trainable = True)\n",
    "\t\t\t\n",
    "\t\tdef call(self, inputs):\n",
    "\t\t\treturn inputs*tf.sigmoid(self.a*(tf.matmul(inputs, self.w) + self.b))\n",
    "\t\n",
    "\twith tf.device(\"/device:GPU:0\"):\n",
    "\t\tmodel = keras.models.Sequential()\n",
    "\t\tif adaptive == \"first\" or adaptive == \"all\":\n",
    "\t\t\tmodel.add(adaptive_dense(layers[1], layers[0]))\n",
    "\t\telse:\n",
    "\t\t\tmodel.add(keras.layers.Dense(layers[1], input_shape = (layers[0], ), activation = \"tanh\"))\n",
    "\t\tfor i in range(1, len(layers)-2):\n",
    "\t\t\tif adaptive == \"all\":\n",
    "\t\t\t\tmodel.add(adaptive_dense(layers[i+1], layers[i]))\n",
    "\t\t\telse:\n",
    "\t\t\t\tmodel.add(keras.layers.Dense(layers[i+1], input_shape = (layers[i], ), activation = \"tanh\"))\n",
    "\t\tif bias:\n",
    "\t\t\tmodel.add(keras.layers.Dense(layers[-1], bias_initializer = keras.initializers.Constant(bias)))\n",
    "\t\telse:\n",
    "\t\t\tmodel.add(keras.layers.Dense(layers[-1]))\n",
    "\treturn model\n",
    "\n",
    "class rectangular_constant_model(keras.Model):\n",
    "\tdef __init__(self, boundary, value):\n",
    "\t\t\"\"\"\n",
    "\t\tboundary: the interfaces of the middle rectangular interval; in ascending order; len(boundary) = 2\n",
    "\t\tvalue_init: initial values of each variable; len(value_init) = 2; here we suppose the value in the first and third interval are the same\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(rectangular_constant_model, self).__init__()\n",
    "\t\tself.boundary = boundary\n",
    "\t\tself.values = [tf.Variable(vi, dtype = tf.float32, trainable = True) for vi in value]\n",
    "\t\t\n",
    "\tdef call(self, inputs): \n",
    "\t\t\"\"\"\n",
    "\t\tinputs: (x, z)\n",
    "\t\toutputs: c'(z)\n",
    "\t\t\"\"\"\n",
    "\t\treturn tf.where(tf.math.logical_and(tf.greater_equal(inputs[:, 1:2], self.boundary[0]), \n",
    "\t\t\t\t\t\t\t\t\t\t\ttf.less_equal(inputs[:, 1:2], self.boundary[1])),\n",
    "\t\t\t\t\t\tself.values[1],\n",
    "\t\t\t\t\t\tself.values[0])\n",
    "\n",
    "w0 = 100\n",
    "c = 20\n",
    "lambda_1 = 0.05\n",
    "\n",
    "direct_layers = [4, 64, 64, 64, 64, 1] # (x, z, t, xs) -> p\n",
    "direct_model = siren_model(direct_layers, c, w0, lambda_1)\n",
    "# direct_model = swish_model(direct_layers, True)\n",
    "\n",
    "x0, z0, t0 = 1e3, 1e3, 1.0\n",
    "p0, u0, v0 = 1.0, 1.0, 1.0\n",
    "c0 = 1e3\n",
    "\n",
    "boundary = np.array([200.0, 300.0]) / z0\n",
    "inverse_values = np.array([1000, 1000]) / c0\n",
    "inverse_model = rectangular_constant_model(boundary, inverse_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, z0, t0 = 1e3, 1e3, 1.0\n",
    "p0, u0, v0 = 1.0, 1.0, 1.0\n",
    "c0 = 1e3\n",
    "\n",
    "# source\n",
    "f_0 = 20.0\n",
    "A = f_0*np.pi\n",
    "alpha = 2.0*10 # multiplied by dx\n",
    "M0 = 1 # scale p\n",
    "\n",
    "@tf.function\n",
    "def R(t, t_s):\n",
    "\treturn M0*(-2*A**2)*(t-t_s)*(3-2*(A*(t-t_s))**2)*tf.exp(-(A*(t-t_s))**2)\n",
    "\n",
    "@tf.function\n",
    "def N(x, z, x_s, z_s):\n",
    "\treturn tf.exp(-1.0/(alpha**2) * ((x-x_s)**2 + (z-z_s)**2))\n",
    "\n",
    "@tf.function\n",
    "def f(x, z, t, x_s, z_s, t_s):\n",
    "\treturn R(t, t_s)*N(x, z, x_s, z_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_zero = 1e-2\n",
    "threshold = 1e-3\n",
    "\n",
    "def weighted_loss(true, pred):\n",
    "\terror = K.square(true - pred)\n",
    "\terror = K.mean(K.switch(K.less_equal(K.abs(true), threshold), w_zero * error , error))\n",
    "\treturn error \n",
    "\n",
    "loss_func = keras.losses.MeanSquaredError()\n",
    "loss_func_r = keras.losses.MeanAbsoluteError() # l1 loss\n",
    "\n",
    "def get_residue_loss(tape, p, c, x, z, t, x_s, z_s, t_s):\n",
    "\tdp_dt = tape.gradient(p, t)\n",
    "\tdp_dx = tape.gradient(p, x)\n",
    "\tdp_dz = tape.gradient(p, z)\n",
    "\tdp_dtt = tape.gradient(dp_dt, t)\n",
    "\tdp_dxx = tape.gradient(dp_dx, x)\n",
    "\tdp_dzz = tape.gradient(dp_dz, z)\n",
    "\treturn loss_func_r(p0/t0**2*dp_dtt, c0**2*c*(p0/x0**2*dp_dxx+p0/z0**2*dp_dzz) + f(x0*x, z0*z, t0*t, x0*x_s, z0*z_s, t0*t_s))\n",
    "\n",
    "def step(data, residue = True):\n",
    "\twith tf.GradientTape(persistent = True) as tape:\n",
    "\t\ttape.watch(data)\n",
    "\t\t[x_c, z_c, t_c, x_sc, z_sc, t_sc,\n",
    "\t\t x_i, z_i, t_i, p_i, x_si, z_si, t_si,\n",
    "\t\t x_d, z_d, t_d, p_d, x_sd, z_sd, t_sd] = data\n",
    "\t\tif residue:\n",
    "\t\t\tout_c = direct_model(tf.concat([x_c, z_c, t_c, x_sc], axis = 1))\n",
    "\t\t\tc_c = inverse_model(tf.concat([x_c, z_c], axis = 1))\n",
    "\t\t\tout_d = direct_model(tf.concat([x_d, z_d, t_d, x_sd], axis = 1))\n",
    "\t\t\tc_d = inverse_model(tf.concat([x_d, z_d], axis = 1))\n",
    "\t\t\tout_i = direct_model(tf.concat([x_i, z_i, t_i, x_si], axis = 1))\n",
    "\t\t\tc_i = inverse_model(tf.concat([x_i, z_i], axis = 1))\n",
    "\t\t\tloss_c = get_residue_loss(tape, out_c[:, 0:1], c_c, x_c, z_c, t_c, x_sc, z_sc, t_sc)\n",
    "\t\t\tloss_dr = get_residue_loss(tape, out_d[:, 0:1], c_d, x_d, z_d, t_d, x_sd, z_sd, t_sd)\n",
    "\t\t\tloss_dv = weighted_loss(out_d[:, 0:1], p_d)\n",
    "\t\t\tloss_ir = get_residue_loss(tape, out_i, c_i, x_i, z_i, t_i, x_si, z_si, t_si)\n",
    "\t\t\tloss_iv = loss_func(out_i[:, 0:1], p_i)\n",
    "\t\telse:\n",
    "\t\t\tout_d = direct_model(tf.concat([x_d, z_d, t_d, x_sd], axis = 1))\n",
    "\t\t\tout_i = direct_model(tf.concat([x_i, z_i, t_i, x_si], axis = 1))\n",
    "\t\t\tloss_c = loss_dr = loss_ir = 0\n",
    "\t\t\tloss_dv = weighted_loss(out_d[:, 0:1], p_d)\n",
    "\t\t\tloss_iv = loss_func(out_i[:, 0:1], p_i)\n",
    "\n",
    "\t\tloss = w_c*loss_c + w_dr*loss_dr + w_dv*loss_dv + w_ir*loss_ir + w_iv*loss_iv\n",
    "\t\n",
    "\tif residue:\n",
    "\t\tgrads = tape.gradient(loss, direct_model.trainable_variables + inverse_model.trainable_variables)\n",
    "\t\toptimizer.apply_gradients(zip(grads, direct_model.trainable_variables + inverse_model.trainable_variables))\n",
    "\telse:\n",
    "\t\tgrads = tape.gradient(loss, direct_model.trainable_variables)\n",
    "\t\toptimizer.apply_gradients(zip(grads, direct_model.trainable_variables))\n",
    "\t\n",
    "\tdel tape\n",
    "\treturn loss, loss_c, loss_dr, loss_dv, loss_ir, loss_iv\n",
    "\n",
    "\n",
    "# data_c: x_c, z_c, t_c, xs_c, z_sc, t_sc\n",
    "# data_i: x_i, z_i, t_i, p_i, u_i, v_i, x_si, z_si, t_si\n",
    "# data_d: x_d, z_d, t_d, p_d, x_sd, z_sd, t_sd\n",
    "def train(residue, data_c, data_i, data_d, start_epoch, epochs, batch_proportion = 0.1, print_every = 100, save_every = 10000, save_path = None):\n",
    "\tshuffled = lambda dataset: [tf.data.Dataset.from_tensor_slices(tuple(d)).shuffle(buffer_size = max(d[0].shape[0], 1), \\\n",
    "                                                        reshuffle_each_iteration = True).batch(max(int(batch_proportion*d[0].shape[0]), 1)) \\\n",
    "                                        for d in dataset]\n",
    "\t\n",
    "\tdata_s = shuffled([data_c, data_i, data_d])\n",
    "\tfor epoch in range(start_epoch+1, start_epoch+epochs+1):\n",
    "\t\tlosses = np.zeros((6,))\n",
    "\t\tfor dc, di, dd in zip(*data_s):\n",
    "\t\t\tloss = step(list(dc) + list(di) + list(dd), residue)\n",
    "\t\t\tlosses += np.array(loss)\n",
    "\t\tlosses *= batch_proportion\n",
    "\t\t\n",
    "\t\tif epoch % print_every == 0:\n",
    "\t\t\tprint(\"{}, Epoch: {}, Loss: {:.4e}, c: {:.4e}, dr: {:.4e}, dv: {:.4e}, ir:{:.4e}, iv: {:.4e}\".format(get_time(), epoch, *list(losses)))\n",
    "\t\t\tprint(inverse_model.values)\n",
    "\t\tif epoch % save_every == 0:\n",
    "\t\t\tdirect_model.save(\"models/{}/{}/checkpoint_{}/direct_model\".format(NAME, save_path, epoch))\n",
    "\t\t\tinverse_model.save(\"models/{}/{}/checkpoint_{}/inverse_model\".format(NAME, save_path, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "x_s = np.linspace(50.0, 450.0, 9) / x0\n",
    "z_s = np.array([10.0]) / z0\n",
    "t_s = np.array([0.05]) / t0\n",
    "n_source = len(x_s)\n",
    "\n",
    "domain = [0, 500]\n",
    "T_max = 0.5\n",
    "dx = 10\n",
    "dt = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cx = n_cz = n_ct = 100 # 50*50*50\n",
    "x_c = np.linspace(*domain, n_cx) / x0\n",
    "z_c = np.linspace(*domain, n_cz) / z0\n",
    "t_c = np.linspace(0, T_max, n_ct) / t0\n",
    "xzts_c = tensor_grid([x_c, z_c, t_c, x_s, z_s, t_s])\n",
    "x_c, z_c, t_c, x_sc, z_sc, t_sc = xzts_c[:, 0:1], xzts_c[:, 1:2], xzts_c[:, 2:3], xzts_c[:, 3:4], xzts_c[:, 4:5], xzts_c[:, 5:6]\n",
    "\n",
    "n_cs = 1000\n",
    "r = transform(np.random.random((n_cs, 1)), 0, dx) / x0\n",
    "theta = transform(np.random.random((n_cs, 1)), 0, 2*np.pi)\n",
    "x_cs = np.vstack([xi + r*np.cos(theta) for xi in x_s])\n",
    "z_cs = np.vstack([z_s[0] + r*np.sin(theta) for i in x_s])\n",
    "t_cs = np.vstack([transform(np.random.random((n_cs, 1)), t_s[0] - 0.5*dt/t0, t_s[0] + 2*dt/t0) for i in x_s])\n",
    "x_scs = np.tile(x_s.reshape((-1, 1)), (1, n_cs)).reshape((-1, 1))\n",
    "z_scs = np.tile(z_s, (n_source, n_cs)).reshape((-1, 1))\n",
    "t_scs = np.tile(t_s, (n_source, n_cs)).reshape((-1, 1))\n",
    "x_c = np.vstack([x_c, x_cs])\n",
    "z_c = np.vstack([z_c, z_cs])\n",
    "t_c = np.vstack([t_c, t_cs])\n",
    "x_sc = np.vstack([x_sc, x_scs])\n",
    "z_sc = np.vstack([z_sc, z_scs])\n",
    "t_sc = np.vstack([t_sc, t_scs])\n",
    "\n",
    "n_i = 200\n",
    "x_i = np.vstack([transform(np.random.random((n_i, 1)), *domain) / x0 for xi in x_s])\n",
    "z_i = np.vstack([transform(np.random.random((n_i, 1)), *domain) / z0 for i in x_s])\n",
    "t_i = np.zeros_like(x_i) / t0\n",
    "p_i = np.zeros_like(x_i) / p0\n",
    "u_i = np.zeros_like(x_i) / u0\n",
    "v_i = np.zeros_like(x_i) / v0\n",
    "x_si = np.tile(x_s.reshape((-1, 1)), (1, n_i)).reshape((-1, 1))\n",
    "z_si = np.tile(z_s, (n_source, n_i)).reshape((-1, 1))\n",
    "t_si = np.tile(t_s, (n_source, n_i)).reshape((-1, 1))\n",
    "\n",
    "map_to_tf_float32 = lambda x: list(map(lambda y: tf.constant(y, dtype = tf.float32), x))\n",
    "\n",
    "n_d = 2500\n",
    "p_d = []\n",
    "for x in x_s:\n",
    "\tp = loadmat(\"data/sr_{}.mat\".format(int(x*x0/dx)))[\"sr_{}\".format(int(x*x0/dx))].reshape((-1, 1)) / p0\n",
    "\tp_d.append(p)\n",
    "\t\n",
    "p_d = np.vstack(p_d)\n",
    "x_d = np.linspace(10.0, 500.0, 50) / x0\n",
    "z_d = np.array([10.0]) / z0\n",
    "t_d = np.linspace(0.0, 0.49, 50) / t0\n",
    "txzs_d = tensor_grid([t_d, x_d, z_d, x_s, z_s, t_s])\n",
    "x_d, z_d, t_d, x_sd, z_sd, t_sd = txzs_d[:, 1:2], txzs_d[:, 2:3], txzs_d[:, 0:1], txzs_d[:, 3:4], txzs_d[:, 4:5], txzs_d[:, 5:6]\n",
    "\n",
    "data_c = map_to_tf_float32([x_c, z_c, t_c, x_sc, z_sc, t_sc])\n",
    "data_i = map_to_tf_float32([x_i, z_i, t_i, p_i, x_si, z_si, t_si])\n",
    "data_d = map_to_tf_float32([x_d, z_d, t_d, p_d, x_sd, z_sd, t_sd])\n",
    "\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "with open(\"dataset_{}.pkl\".format(NAME), \"wb\") as file:\n",
    "\tpickle.dump([data_c, data_i, data_d], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import cPickle as pickle\n",
    "\n",
    "with open(\"dataset_{}.pkl\".format(NAME), \"rb\") as file:\n",
    "\tdata_c, data_i, data_d = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "optimizer = keras.optimizers.Adam(learning_rate = keras.optimizers.schedules.PiecewiseConstantDecay([1000*i for i in range(3)], \n",
    "\t\t\t\t\t\t\t\t\t\t[1e-3*0.5**i for i in range(4)]))\n",
    "\n",
    "w_c = 1.0\n",
    "w_dr = 1.0\n",
    "w_ir = 1.0\n",
    "w_dv = 1e5\n",
    "w_iv = 1e2\n",
    "\n",
    "w_zero = 0.001\n",
    "threshold = 1e-4\n",
    "\n",
    "train(True, data_c, data_i, data_d, \n",
    "      start_epoch = 0, epochs = 4000, \n",
    "      batch_proportion = 0.005, print_every = 1, save_every = 50, save_path = \"{}_adam\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct_model = keras.models.load_model(\"models/{}/{}/checkpoint_{}/direct_model\".format(NAME, \"1_adam\", 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 2\n",
    "optimizer = keras.optimizers.Adam(learning_rate = keras.optimizers.schedules.PiecewiseConstantDecay([1000], \n",
    "\t\t\t\t\t\t\t\t\t\t[1e-4, 1e-5]))\n",
    "\n",
    "w_c1, w_c2, w_c3 = 1.0, 1.0, 1.0\n",
    "w_dr1, w_dr2, w_dr3 = 1.0, 1.0, 1.0\n",
    "w_ir1, w_ir2, w_ir3 = 1.0, 1.0, 1.0\n",
    "w_dv = 1e5\n",
    "w_iv = 1e2\n",
    "\n",
    "w_zero = 0.1\n",
    "threshold = 1e-3\n",
    "\n",
    "train(True, data_c, data_i, data_d, \n",
    "      start_epoch = 0, epochs = 2000, \n",
    "      batch_proportion = 0.02, print_every = 2, save_every = 50, save_path = \"{}_adam\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.linspace(1.0, 500.0, 500) / x0\n",
    "z_test = np.linspace(1.0, 500.0, 500) / z0\n",
    "xz_test = tensor_grid([z_test, x_test])\n",
    "xz_test[:, [0, 1]] = xz_test[:, [1, 0]]\n",
    "\n",
    "c_pred = inverse_model(tf.constant(xz_test, dtype = tf.float32), training = False)*c0\n",
    "def c_func(x, z):\n",
    "\treturn np.piecewise(z, [z > 300, (z >= 200) & (z <= 300), z < 200], [1500, 3500, 1500])\n",
    "c_true = c_func(xz_test[:, 0:1]*x0, xz_test[:, 1:2]*z0)\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib.cm import cool\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "cmap = cool\n",
    "norm = Normalize(vmin=1000, vmax=5000)\n",
    "\n",
    "X, Z = np.meshgrid(x_test, z_test)\n",
    "fig, ax = plt.subplots(1, 3, figsize = (15, 5))\n",
    "fig.subplots_adjust(right = 1.0)\n",
    "\n",
    "im0 = ax[0].contourf(X, Z, c_true.reshape((len(z_test), len(x_test))), cmap = cmap, norm = norm)\n",
    "ax[0].set_title(\"true\")\n",
    "divider = make_axes_locatable(ax[0])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "mpl.colorbar.ColorbarBase(cax, cmap = cmap, norm = norm, orientation='vertical')\n",
    "\n",
    "norm = mpl.colors.Normalize(vmin=0, vmax=5000)\n",
    "im1 = ax[1].contourf(X, Z, c_pred.numpy().reshape((len(z_test), len(x_test))), cmap = cmap, norm = norm)\n",
    "ax[1].set_title(\"pred\")\n",
    "ax[1].set_xlabel(\"\")\n",
    "divider = make_axes_locatable(ax[1])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "mpl.colorbar.ColorbarBase(cax, cmap = cmap, norm = norm, orientation='vertical')\n",
    "\n",
    "norm = mpl.colors.Normalize(vmin=-1e-2, vmax=1e-2)\n",
    "im2 = ax[2].contourf(X, Z, c_true.reshape((len(z_test), len(x_test)))-c_pred.numpy().reshape((len(z_test), len(x_test))), cmap = cmap, norm = norm)\n",
    "ax[2].set_title(\"error\")\n",
    "divider = make_axes_locatable(ax[2])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "mpl.colorbar.ColorbarBase(cax, cmap = cmap, norm = norm, orientation='vertical')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(c_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(c_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = 5\n",
    "p = loadmat(\"data/sr_{}.mat\".format(int(x_s[i]*x0/dx)))[\"sr_{}\".format(int(x_s[i]*x0/dx))]\n",
    "\n",
    "x_test = np.linspace(10.0, 500.0, 50) / x0\n",
    "z_test = np.array([10]) / z0\n",
    "t_test = np.linspace(0.0, 0.49, 50) / t0\n",
    "xzt_test = tensor_grid([t_test, x_test, z_test])\n",
    "xzt_test[:, [0, 1, 2]] = xzt_test[:, [1, 2, 0]]\n",
    "\n",
    "xzts = np.hstack([xzt_test, x_s[i]*np.ones_like(xzt_test[:, 0:1])])\n",
    "out = direct_model(tf.constant(xzts, dtype = tf.float32))\n",
    "\n",
    "cmap = cool\n",
    "norm = Normalize(vmin=-0.2, vmax=0.2)\n",
    "\n",
    "X, T = np.meshgrid(x_test, t_test)\n",
    "fig, ax = plt.subplots(1, 3, figsize = (15, 5))\n",
    "fig.subplots_adjust(right = 1.0)\n",
    "\n",
    "im0 = ax[0].contourf(X, T, p, cmap = cmap, norm = norm, levels = 1000)\n",
    "ax[0].set_title(\"p, true\")\n",
    "divider = make_axes_locatable(ax[0])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "mpl.colorbar.ColorbarBase(cax, cmap = cmap, norm = norm, orientation='vertical')\n",
    "\n",
    "norm = Normalize(vmin=-0.2, vmax=0.2)\n",
    "im1 = ax[1].contourf(X, T, out[:, 0:1].numpy().reshape((len(t_test), len(x_test))), cmap = cmap, norm = norm, levels = 1000)\n",
    "ax[1].set_title(\"p, pred\")\n",
    "ax[1].set_xlabel(\"\")\n",
    "divider = make_axes_locatable(ax[1])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "mpl.colorbar.ColorbarBase(cax, cmap = cmap, norm = norm, orientation='vertical')\n",
    "\n",
    "norm = mpl.colors.Normalize(vmin=-1e-3, vmax=1e-3)\n",
    "im2 = ax[2].contourf(X, T, p-out[:, 0:1].numpy().reshape((len(t_test), len(x_test))), cmap = cmap, norm = norm, levels = 1000)\n",
    "ax[2].set_title(\"p, error\")\n",
    "divider = make_axes_locatable(ax[2])\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "mpl.colorbar.ColorbarBase(cax, cmap = cmap, norm = norm, orientation='vertical')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "\n",
    "x_test = np.linspace(10.0, 500.0, 50) / x0\n",
    "z_test = np.linspace(10.0, 500.0, 50) / z0\n",
    "t_test = np.array([0]) / t0\n",
    "xzt_test = tensor_grid([t_test, x_test, z_test])\n",
    "xzt_test[:, [0, 1, 2]] = xzt_test[:, [1, 2, 0]]\n",
    "\n",
    "xzts = np.hstack([xzt_test, x_s[i]*np.ones_like(xzt_test[:, 0:1])])\n",
    "out = direct_model(tf.constant(xzts, dtype = tf.float32))\n",
    "\n",
    "cmap = cool\n",
    "norm = Normalize(vmin=-0.1, vmax=0.1)\n",
    "\n",
    "X, Z = np.meshgrid(x_test, z_test)\n",
    "fig, ax = plt.subplots(1, 1, figsize = (5, 5))\n",
    "fig.subplots_adjust(right = 1.0)\n",
    "\n",
    "im0 = ax.contourf(X, Z, out[:, 0:1].numpy().reshape((len(z_test), len(x_test))), cmap = cmap, norm = norm, levels = 1000)\n",
    "ax.set_title(\"p, pred, t=0\")\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes('right', size='5%', pad=0.05)\n",
    "mpl.colorbar.ColorbarBase(cax, cmap = cmap, norm = norm, orientation='vertical')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
